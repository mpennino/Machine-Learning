---
title: "RFC"
author: "Michael Pennino"
date: "March 16, 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---


# Inputs
```{r, echo=F, eval=T, message=FALSE, warning=FALSE}
library(randomForest)


data_dir = '...'
m_dir = '...'
D_dir = '...'

# Importing Data

# All 93,000+ catchments with PWSs that are found only in 1 catchment
cat.PWS.All.Vars = readRDS(paste0(data_dir,"ArcGIS/SDWIS_Model/RandomForest/Catchments_93K_PWS_All_Variables.rds"))

# "93k" cats with just GW PWSs
cat95in1_gw = read.csv(paste0(D_dir,"Data/cat95in1_gw.csv"))
cat95in1_gw = as.data.frame(cat95in1_gw[,'COMID'])
names(cat95in1_gw)[1] = 'COMID'

# Streamcat variables for all 2.6 Million conterminous U.S. catchments
#Cat.Viol.Freq.All.SE = readRDS(paste0(data_dir,"ArcGIS/SDWIS_Model/RandomForest/Catchments_Viol_Freq_All.SE.rds")) 

# Has 5076 catchments with PWS NO3 violators and PWS with violations but no NO3 violations
Cat.Viol.Freq.T.All.Vars.GW = readRDS(paste0(data_dir,"ArcGIS/SDWIS_Model/RandomForest/RF_Catchments_Viol_Freq_All_Variables_GW.rds"))



out_dir <- paste0(m_dir,"R/Scripts/Spatial_R/loc_data/")
```

# Remove Duplicate Variables
* Subset out variables that are correlated due to having the same data but for different years
* Removing earlier year (i.e. keeping 2011 instead of 2006)
* Also remove PctIce because this is all zero for most or all catchments
```{r, eval=T, echo=T}

# Make a table of number of NAs for each variable (column)
na_count <-sapply(Cat.Viol.Freq.T.All.Vars.GW, 
                  function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)

rmov = c(# These have a lot of zeros
        'PctIce2011Cat','PctIce2011Ws',
        'S_TW2012Cat','S_TW2012Ws',
        'Pop.Served.avg','Pop.Served.km2',
        'MWST_2014','MAST_2014','MSST_2014'
        #,'Precip08Cat','Precip08Ws','Tmean08Cat','Tmean08Ws'
        )
        

Cat.Viol.Freq.T.All.Vars.GW2 = 
  Cat.Viol.Freq.T.All.Vars.GW[,!(names(Cat.Viol.Freq.T.All.Vars.GW) %in% rmov)]

cat.PWS.All.Vars2 = cat.PWS.All.Vars[,!(names(cat.PWS.All.Vars) %in% rmov)]

na_count <-sapply(Cat.Viol.Freq.T.All.Vars.GW2, 
                  function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)

```

# Prep Model on Full Dataset
* Classification, with balancing
```{r, eval=T, echo=T}
# Remove the ID column (COMID)
cl_GW = subset(Cat.Viol.Freq.T.All.Vars.GW2, select = -c(COMID))

# Change the contuous Violaton.Freq variable to binary variable (1 or 0)
cl_GW$Viol.Class = ifelse(cl_GW$Viol.Freq > 0,1,0)
cl_GW2 = subset(cl_GW, select = -c(Viol.Freq)) # remove column
cl_GW2 = na.omit(cl_GW2)

# used for balancing in RF model by finding variable with least number of observations
cmin <- min(table(cl_GW2$Viol.Class)) 

nrow(cl_GW2) # 60862
cmin # 748


names(cl_GW2)[which(colnames(cl_GW2)=="GWAVA_outWS")] = "GWAVA_outWs"


```




# Independent test and training datasets
* Classification, with balancing
```{r, eval=T, echo=T}


set.seed(100)
# Subsetting Test set (10% of Data Set)
ind_test <- sample(nrow(cl_GW2), round(nrow(cl_GW2)*0.1))
test_class = cl_GW2[ind_test, ]
train_class = cl_GW2[-ind_test,]

# checks
nrow(test_class) # 684 
nrow(train_class) # 6155
nrow(test_class)+nrow(train_class) # 6839

#####################

# Obtains minimum number of rows each class in order to run balanced RF model 
cmin <- min(table(train_class$Viol.Class))

```


#***********************************************************************
# RF with hypothesized variables
* This selects out several variables that I hypothesize to have the most influence on the response variable
```{r, eval=T, eco=T}

set.seed(100)

hypo_vars = c(# Updated
              'Viol.Class','TREATMENT',
              'FertCat','FertWs',
              'PctCrop2011Cat','PctCrop2011Ws',
              'WtDepCat','WtDepWs',
              'PermCat','PermWs',
              'HydrlCondCat','OmCat',
              'RunoffCat','RunoffWs',
              'ManureCat','ManureWs',
              'NO3_2008Cat','NO3_2008Ws',
              'InorgNWetDep_2008Cat','InorgNWetDep_2008Ws',
              
               "IncomePerCap", "Avg_Neighbor_Income",
               "Perc_White" ,"Perc_Black","Perc_Minority",
               "Perc_BelowPoverty","Perc_HighSchool",
               
               "WaterInputCat",'WaterInputWs' ,
               'Hillslope_PctCat','Hillslope_PctWs',
               'AgDrain_pctCat','AgDrain_pctWs',
               "pptSurpCat","pptSurpWs",
               "wwtp_all_km2Cat", "wwtp_all_km2Ws",
               "wwtp_major_km2Cat","wwtp_major_km2Ws",
               "wwtp_minor_km2Cat", "wwtp_minor_km2Ws",
               'Septic_km2Cat','Septic_km2Ws',
               'Nsurp_kg_Cat','Nsurp_kg_Ws',
               'Nsurp_kgkm2_Cat','Nsurp_kgkm2_Ws',
               'NANI_kg_Cat','NANI_kg_Ws',
               'NANI_kgkm2_Cat','NANI_kgkm2_Ws',
               'sw_loadCat','sw_loadWs','sw_fluxCat','sw_fluxWs',
              
               "PctUSGACat","PctSSACat"
              )

# Select only specific variables:
train_class_hyp = train_class[,hypo_vars]

test_class_hyp = test_class[,hypo_vars]

ncol(train_class) # 224
ncol(train_class_hyp) # 16

# Obtains minimum number of rows each class in order to run balanced RF model 
cmin <- min(table(train_class_hyp$Viol.Class))



```


#***********************************************************************
########################################################################


# RFC - CV - 90% Balanced
* Running Random Forest Classification Model
* Prebalancing the zero inflated dataset (by under sampling the majority class similar to Anand et al. 2010 and Dal Pozzalo et al. 2015).
* Doing a 10-fold cross validation with 90% as training dataset and 10% as testing dataset
```{r}

set.seed(22)

# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_GW2[cl_GW2$Viol.Class == 0,] # separate cats without viols
  viol = cl_GW2[cl_GW2$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 4.888982 mins





#**********************************************************************
# PREDICT ON FULL DATASET
#**********************************************************************

# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- cl_GW2$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = cl_GW2)
#   n <- nrow(cl_GW2)
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc)
# mean(sens) 
# mean(spec)
# sqrt(mean(sens)*mean(spec)) # 75.50967


#**********************************************************************
# Convert Predicted Probabilities to Actual Probabilities
#**********************************************************************
# ** USe this for predicting on unbalanced full or holdout datasets
p = beta * p_s / ((beta-1) * p_s + 1) 

# Where: 
# p_s = predicted probability
# beta =  ratio of the number majority class instances after undersampling over the number majority class ones in the original training set.


#**********************************************************************
# PREDICT ON FULL DATASET - Bias Corrected
#**********************************************************************

# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- cl_GW2$Viol.Class
#   beta = nrow(cl_GW2[cl_GW2$Viol.Class == 1,]) /
#     nrow(cl_GW2[cl_GW2$Viol.Class == 0,])
#   p_s_ = predict(RFC_model[[i]],newdata = cl_GW2)
#   p_s = as.numeric(as.character(p_s_))
#   ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
#   ypred = ifelse(ypred1 > 0.5,1,0)
#   n <- nrow(cl_GW2)
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc)
# mean(sens) # 96.15385
# mean(spec)
# sqrt(mean(sens)*mean(spec))

#**********************************************************************
# PREDICT ON TESTING DATASET
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc) #  75.57553
mean(sens,na.rm=T) # 74.21622
mean(spec) # 77.03784
sqrt(mean(sens)*mean(spec)) # 75.50967

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_balanced90.rds'))
saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_balanced90.rds'))
saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_balanced90.rds'))


```

#* Results90
* This calculates the classification results for predictions on the testing dataset, such as 
*   PCC = % correctly classified
*   Sensitivity = % of positive class correclty classified
*   Specificity = % of negative class correctly classified
*   AUC = area under the curve.
*   gmean = sqrt(sensitivity * specificity) helps determine how balanced the model is.  

```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_balanced90.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_balanced90.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_balanced90.rds'))

#**********************************************************************
rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]
# 
# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
library(data.table)
#varImpPlot(RF_All) 
varImpPlot(rfc1) 
varImpPlot(rfc2) 


# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])


# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 

View(RI3)



#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
#RFC_All  # Confusion matrix only works for the individual, not combined RF objects
rfc1
rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TRAINERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
#   n <- nrow(TRAINERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_train = mean(pcc)
# sens_train = mean(sens)
# spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

# For Test Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
  AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)


#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        gmean_test = gmean_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$gmean_test = round(CV_Results$gmean_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

View(CV_Results)

saveRDS(RI3, file=paste0(out_dir, 'TopImp_RFC_GW_Bal90.rds'))

```

# RF - TopImp90
* Taking the variable importance rankings and finding out if the model is improved by only keeping top N variables
* This code uses a loop to re-run the model using the first 2, then 3, then 4... N variables to see which gives the most accurate predictions.  
```{r}
start_time1 <- Sys.time()
RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_Bal90.rds'))

library(randomForest)

topImpTable = data.frame(ImpVar_Num = 0, Gmean = 0)
for(topImp in 1:100) { # using 100 instead of nrow(RI3) so that it won't take so long

# Use top 10 important variables from balanced model above
hypo_vars = as.character(RI3[1:topImp,]$Variable)

new_df = cl_GW2[,c('Viol.Class',hypo_vars)]

set.seed(22)
# Run the Model
RF_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  
  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp   # comment this out if using the orignal way
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE) 
}
end_time <- Sys.time(); end_time - start_time
#Time difference of 2.479181 secs, hypo 

pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}


topImpTable[topImp,1] = topImp
topImpTable[topImp,2] = sqrt(mean(sens)*mean(spec))

}


end_time1 <- Sys.time(); end_time1 - start_time1
#Time difference of 12.8062 hours


```

# RF - TopImpFinal90
* Based on the results from the above chunk, only the top N variables are used to re-run the model 
```{r}

library(randomForest)
RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_Bal90.rds'))

# Number of Top Imiportant Variables to use
topImp = 86

# Use top 10 important variables from balanced model above
hypo_vars = as.character(RI3[1:topImp,]$Variable)

new_df = cl_GW2[,c('Viol.Class',hypo_vars)]


set.seed(22)
# Run the Model
RF_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
   # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp   # comment this out if using the orignal way
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE) 
}
end_time <- Sys.time(); end_time - start_time
#Time difference of 2.479181 secs, hypo 

# Testing Set
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}

#Gmean 
sqrt(mean(sens)*mean(spec))

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_Imp90.rds'))
saveRDS(TRAINERS, file=paste0(out_dir, 'TRAINERS_RFC_CV_GW_Imp90.rds'))
saveRDS(TESTERS, file=paste0(out_dir, 'TESTERS_RFC_CV_GW_Imp90.rds'))


```

#* Results 90%
* This calculates the classification results for predictions on the testing dataset, such as 
*   PCC = % correctly classified
*   Sensitivity = % of positive class correclty classified
*   Specificity = % of negative class correctly classified
*   AUC = area under the curve.
*   gmean = sqrt(sensitivity * specificity) helps determine how balanced the model is.  

```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_Imp90.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_Imp90.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_Imp90.rds'))

#**********************************************************************

rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]
# 
# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
library(data.table)
#varImpPlot(RF_All) 
varImpPlot(rfc1) 
varImpPlot(rfc2) 


# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])


# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 

View(RI3)




#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
#RFC_All  # Confusion matrix only works for the individual, not combined RF objects
rfc1
rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)
gmean_train = sqrt(sens_train*spec_train)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
  
  AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.870137877019063


#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        gmean_test = gmean_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$gmean_test = round(CV_Results$gmean_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

View(CV_Results)

saveRDS(RI3, file=paste0(out_dir, 'TopImp_RFC_GW_BalFinal90.rds'))


```

#**
# RFC-CV-Bal-Hold90
* This is an additional check on how well the model can make predictions on the unbalanced original dataset
* This method removes 20% of the data prior to doing the 10-fold cross validation and then uses the resulting model to make predictions on the holdout data, which was never used in training the model
```{r}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Make a 20% hold out dataset
set.seed(25)
cl_GW2$ROWID = seq.int(nrow(cl_GW2)) # add rowid
df_80 = cl_GW2[sample(nrow(cl_GW2), 0.8*nrow(cl_GW2)), ]
nrow(cl_GW2)
nrow(df_80)
nrow(cl_GW2[cl_GW2$Viol.Class>0,]) # 26
nrow(df_80[df_80$Viol.Class>0,]) #  13

selectedRows  = (!cl_GW2$ROWID %in% df_80$ROWID) # gives rows for all matching
df_20 = cl_GW2[selectedRows,]
nrow(df_20) # 2040
nrow(df_20[df_20$Viol.Class>0,]) #  13
cl_GW2 = subset(cl_GW2, select=-c(ROWID)) # remove ROWID
df_80 = subset(df_80, select=-c(ROWID)) # remove ROWID
df_20 = subset(df_20, select=-c(ROWID)) # remove ROWID

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


set.seed(22)


# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = df_80[df_80$Viol.Class == 0,] # separate cats without viols
  viol = df_80[df_80$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 80% training & 20% test datasets based on CV_category
  # if(i < 10) { # for i = 1:9
  #   subtemp = sub[sub$CV_category != i & sub$CV_category != (i+1),]
  #   subtemp = subset(subtemp, select=-c(CV_category))
  #   } else {  # for i = 10
  #   subtemp = sub[sub$CV_category != i & sub$CV_category != 1,]
  #   subtemp = subset(subtemp, select=-c(CV_category))
  #   }
  # if(i < 10) { # for i = 1:9
  #   subtest = sub[sub$CV_category == i | sub$CV_category == (i+1),]
  #   subtest = subset(subtest, select=-c(CV_category))
  #   } else {  # for i = 10
  #   subtest = sub[sub$CV_category == i | sub$CV_category == 1,]
  #   subtest = subset(subtest, select=-c(CV_category))
  #   }
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.621551 mins


#**********************************************************************
# PREDICT ON TESTING DATASET
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc) #  75.57553
mean(sens,na.rm=T) # 74.57576
mean(spec) # 76.56061
sqrt(mean(sens)*mean(spec))

#**********************************************************************
# PREDICT ON FULL DATASET
#**********************************************************************

# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- cl_GW2$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = cl_GW2)
#   n <- nrow(cl_GW2)
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc) # 77.16583
# mean(sens) # 89.11765
# mean(spec) # 77.0068
# sqrt(mean(sens)*mean(spec))

#**********************************************************************
# PREDICT ON HOLDOUT DATASET - bias Corrected
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc)
mean(sens)
mean(spec)
sqrt(mean(sens)*mean(spec))

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_bal_hold90.rds'))
saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_bal_hold90.rds'))
saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_bal_hold90.rds'))
saveRDS(df_20, file=paste0(out_dir,'df_20_RFC_CV_GW_bal_hold90.rds'))

```

#* Results90
```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_bal_hold90.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_bal_hold90.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_bal_hold90.rds'))
df_20 <- readRDS(paste0(out_dir, 'df_20_RFC_CV_GW_bal_hold90.rds'))

#**********************************************************************
rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]

# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
# library(data.table)
# #varImpPlot(RF_All) 
# varImpPlot(rfc1) 
# varImpPlot(rfc2) 
# 
# 
# # make a list of all variable importance tables
# imprtnce = list()
# for (i in 1:10){ 
#   imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
#   
#   names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
#   setDT(imprtnce[[i]], keep.rownames = TRUE)[]
#   imprtnce[[i]] = imprtnce[[i]][,1:2]
# }
# 
# # merge variables together
# imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')
# 
# names(imprtnce_all)[1] = 'Variable'
# 
# # Get average Relative Importance:
# imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])
# 
# # Merge on Short Variable Names
# varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')
# 
# 
# # Sort
# RI <- imprtnce_all[order(-Relative_Importance),] 
# RI = RI[,c('Variable','Relative_Importance')]
# 
# RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
#             by.x='Variable', by.y='Variable.Name', all.x=T)
# RI3 <- RI2[order(-Relative_Importance),] 
# 
# View(RI3)



#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
# RFC_All  # Confusion matrix only works for the individual, not combined RF objects
# rfc1
# rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TRAINERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
#   n <- nrow(TRAINERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_train = mean(pcc)
# sens_train = mean(sens)
# spec_train = mean(spec)

# TESTING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TESTERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
#   n <- nrow(TESTERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_test = mean(pcc)
# sens_test = mean(sens)
# spec_test = mean(spec)
# gmean_test = sqrt(sens_test*spec_test)

#**********************************************************************
# PREDICT ON HOLDOUT DATASET - bias Corrected
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_hold = mean(pcc)
sens_hold = mean(sens)
spec_hold = mean(spec)
gmean_hold = sqrt(sens_hold*spec_hold)


#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


# For Test Dataset
# AUC = c()
# for (i in 1:10){ 
#   cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
#   AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
# }
# AUC_all = mean(AUC)


# For Holdout Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = df_20, type='prob')
  AUC[i] = roc.area(cl_pred[,2], df_20$Viol.Class) 
}
AUC_hold = mean(AUC)

#*******************************************************
# Make Table of Results
# CV_Results = data.frame(pcc_test = pcc_test,
#                         sens_test = sens_test,
#                         spec_test = spec_test,
#                         gmean_test = gmean_test,
#                         AUC_all = AUC_all)
#                        
# CV_Results$pcc_test = round(CV_Results$pcc_test,1)
# CV_Results$sens_test = round(CV_Results$sens_test,1)
# CV_Results$spec_test = round(CV_Results$spec_test,1)
# CV_Results$gmean_test = round(CV_Results$gmean_test,1)
# CV_Results$AUC_all = round(CV_Results$AUC_all,2)

# Holdout Results
CV_hold = data.frame(pcc_hold = pcc_hold,
                        sens_hold = sens_hold,
                        spec_hold = spec_hold,
                        gmean_hold = gmean_hold,
                        AUC_hold = AUC_hold)
                       
CV_hold$pcc_hold = round(CV_hold$pcc_hold,1)
CV_hold$sens_hold = round(CV_hold$sens_hold,1)
CV_hold$spec_hold = round(CV_hold$spec_hold,1)
CV_hold$gmean_hold = round(CV_hold$gmean_hold,1)
CV_hold$AUC_hold = round(CV_hold$AUC_hold,2)

#View(CV_Results)
View(CV_hold)

saveRDS(RI3, file=paste0(out_dir, 'TopImp_RFC_GW_Hold90.rds'))

```

# RF-Hold-TopImp90
```{r}
start_time1 <- Sys.time()
RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_Hold90.rds'))

library(randomForest)

topImpTable = data.frame(ImpVar_Num = 0, Gmean = 0)
for(topImp in 1:100) {
  
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Make a 20% hold out dataset
set.seed(22)
cl_GW2$ROWID = seq.int(nrow(cl_GW2)) # add rowid
df_80 = cl_GW2[sample(nrow(cl_GW2), 0.8*nrow(cl_GW2)), ]
nrow(cl_GW2)
nrow(df_80)
nrow(cl_GW2[cl_GW2$Viol.Class>0,]) # 26
nrow(df_80[df_80$Viol.Class>0,]) #  13

selectedRows  = (!cl_GW2$ROWID %in% df_80$ROWID) # gives rows for all matching
df_20 = cl_GW2[selectedRows,]
nrow(df_20) # 2040
nrow(df_20[df_20$Viol.Class>0,]) #  13
cl_GW2 = subset(cl_GW2, select=-c(ROWID)) # remove ROWID
df_80 = subset(df_80, select=-c(ROWID)) # remove ROWID
df_20 = subset(df_20, select=-c(ROWID)) # remove ROWID

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Use top X important variables from balanced model above
hypo_vars = as.character(RI3[1:topImp,]$Variable)

df_80 = df_80[,c('Viol.Class',hypo_vars)]
df_20 = df_20[,c('Viol.Class',hypo_vars)]

set.seed(22)
# Run the Model
RF_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = df_80[df_80$Viol.Class == 0,] # separate cats without viols
  viol = df_80[df_80$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp   # comment this out if using the orignal way
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE) 
}
end_time <- Sys.time(); end_time - start_time
#Time difference of 2.479181 secs, hypo 

#**********************************************************************
# PREDICT ON HOLDOUT DATASET - Corrected
#**********************************************************************

pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}

topImpTable[topImp,1] = topImp
topImpTable[topImp,2] = sqrt(mean(sens)*mean(spec))

}


end_time1 <- Sys.time(); end_time1 - start_time1
# Time difference of 1.455391 hours
#86 top important  variables 

```

# RF-Hold-TopImpFinal90%
```{r}

RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_Hold90.rds'))


topImp = 71 # based on above chunk, then 78, then 84
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Make a 20% hold out dataset
set.seed(22)
cl_GW2$ROWID = seq.int(nrow(cl_GW2)) # add rowid
df_80 = cl_GW2[sample(nrow(cl_GW2), 0.8*nrow(cl_GW2)), ]
nrow(cl_GW2)
nrow(df_80)
nrow(cl_GW2[cl_GW2$Viol.Class>0,]) # 26
nrow(df_80[df_80$Viol.Class>0,]) #  13

selectedRows  = (!cl_GW2$ROWID %in% df_80$ROWID) # gives rows for all matching
df_20 = cl_GW2[selectedRows,]
nrow(df_20) # 2040
nrow(df_20[df_20$Viol.Class>0,]) #  13
cl_GW2 = subset(cl_GW2, select=-c(ROWID)) # remove ROWID
df_80 = subset(df_80, select=-c(ROWID)) # remove ROWID
df_20 = subset(df_20, select=-c(ROWID)) # remove ROWID

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

hypo_vars = as.character(RI3[1:topImp,]$Variable)

df_80 = df_80[,c('Viol.Class',hypo_vars)]
df_20 = df_20[,c('Viol.Class',hypo_vars)]

set.seed(22)


# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = df_80[df_80$Viol.Class == 0,] # separate cats without viols
  viol = df_80[df_80$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.42233 secs



#**********************************************************************
# Convert Predicted Probabilities to Actual Probabilities
#**********************************************************************
# ** USe this for predicting on unbalanced full or holdout datasets
#  p = beta * p_s / ((beta-1) * p_s + 1) 

# Where: 
# p_s = predicted probability
# beta =  ratio of the number majority class instances after undersampling over the number majority class ones in the original training set.


#**********************************************************************
# PREDICT ON HOLDOUT DATASET - Corrected
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc) # 72.91188
mean(sens) # 72.5
mean(spec) # 72.91369
sqrt(mean(sens)*mean(spec))

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_bal_holdImp90.rds'))
saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_bal_holdImp90.rds'))
saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_bal_holdImp90.rds'))
saveRDS(df_20, file=paste0(out_dir,'df_20_RFC_CV_GW_bal_holdImp90.rds'))


```

#* Results 90%
```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_bal_holdImp90.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_bal_holdImp90.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_bal_holdImp90.rds'))
df_20 <- readRDS(paste0(out_dir, 'df_20_RFC_CV_GW_bal_holdImp90.rds'))

#**********************************************************************

rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]
# 
# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
library(data.table)
#varImpPlot(RF_All) 
varImpPlot(rfc1) 
varImpPlot(rfc2) 


# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])


# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')


# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 

#View(RI3)





#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
#RFC_All  # Confusion matrix only works for the individual, not combined RF objects
rfc1
rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)


# TRAINING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TRAINERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
#   n <- nrow(TRAINERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_train = mean(pcc)
# sens_train = mean(sens)
# spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#**********************************************************************
# PREDICT ON HOLDOUT DATASET - bias Corrected
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_hold = mean(pcc)
sens_hold = mean(sens)
spec_hold = mean(spec)
gmean_hold = sqrt(sens_hold*spec_hold)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

# For Test Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
  AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.870137877019063

# For Holdout Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = df_20, type='prob')
  AUC[i] = roc.area(cl_pred[,2], df_20$Viol.Class) 
}
AUC_hold = mean(AUC)



#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        gmean_test = gmean_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$gmean_test = round(CV_Results$gmean_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

# Holdout Results
CV_hold = data.frame(pcc_hold = pcc_hold,
                        sens_hold = sens_hold,
                        spec_hold = spec_hold,
                        gmean_hold = gmean_hold,
                        AUC_hold = AUC_hold)
                       
CV_hold$pcc_hold = round(CV_hold$pcc_hold,1)
CV_hold$sens_hold = round(CV_hold$sens_hold,1)
CV_hold$spec_hold = round(CV_hold$spec_hold,1)
CV_hold$gmean_hold = round(CV_hold$gmean_hold,1)
CV_hold$AUC_hold = round(CV_hold$AUC_hold,2)

View(CV_Results)
View(CV_hold)

saveRDS(RI3, file=paste0(out_dir, 'TopImp_RFC_GW_Hold90.rds'))

HoldImp <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_Hold90.rds'))


```
#**

# RFC-FINAL-BAL-MODEL
* This final does not do the 10-fold cross validation, it just used the top important variables from the balanced model above and trains the model with all of the data (no holdout)
* This final model is what is used to create the final variable importance and partial dependence plot figures.  
* This final model is also used for making the final prediction maps.
```{r}


set.seed(23)

# Using Top Important Variables from Balanced Model Above without Holdout
RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_BalFinal90.rds'))

hypo_vars = as.character(RI3$Variable)

cl_final = cl_GW2

# Use this if not using all variables, but just ones selected from TopImp
cl_final = cl_final[,c('Viol.Class',hypo_vars)]


# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_final[cl_final$Viol.Class == 0,] # separate cats without viols
  viol = cl_final[cl_final$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  TRAINERS[[i]] = sub

  # Run the model
  cmin_ <- min(table(sub$Viol.Class))
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = sub, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = sub$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time


#**********************************************************************
# PREDICT ON TRAINING DATASET
#**********************************************************************
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TRAINERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
#   n <- nrow(TRAINERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc)
# mean(sens)
# mean(spec)
# sqrt(mean(spec)*mean(spec))

#**********************************************************************
# PREDICT ON FULL DATASET
#**********************************************************************

pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- cl_GW2$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = cl_GW2)
  n <- nrow(cl_GW2)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc) # 73.79461
mean(sens) # 89.61538
mean(spec) # 73.70403
sqrt(mean(sens)*mean(spec)) # 86.58883

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_FINAL_bal.rds'))
saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_FINAL_bal.rds'))
saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_FINAL_bal.rds'))


```

#* My Combine
```{r}
# This is used to combine different random forest models with different sizes
# from here: https://stackoverflow.com/questions/19170130/combining-random-forests-built-with-different-training-sets-in-r
my_combine <- function (...) 
{
    pad0 <- function(x, len) c(x, rep(0, len - length(x)))
    padm0 <- function(x, len) rbind(x, matrix(0, nrow = len - 
        nrow(x), ncol = ncol(x)))
    rflist <- list(...)
    areForest <- sapply(rflist, function(x) inherits(x, "randomForest"))
    if (any(!areForest)) 
        stop("Argument must be a list of randomForest objects")
    rf <- rflist[[1]]
    classRF <- rf$type == "classification"
    trees <- sapply(rflist, function(x) x$ntree)
    ntree <- sum(trees)
    rf$ntree <- ntree
    nforest <- length(rflist)
    haveTest <- !any(sapply(rflist, function(x) is.null(x$test)))
    vlist <- lapply(rflist, function(x) rownames(importance(x)))
    numvars <- sapply(vlist, length)
    if (!all(numvars[1] == numvars[-1])) 
        stop("Unequal number of predictor variables in the randomForest objects.")
    for (i in seq_along(vlist)) {
        if (!all(vlist[[i]] == vlist[[1]])) 
            stop("Predictor variables are different in the randomForest objects.")
    }
    haveForest <- sapply(rflist, function(x) !is.null(x$forest))
    if (all(haveForest)) {
        nrnodes <- max(sapply(rflist, function(x) x$forest$nrnodes))
        rf$forest$nrnodes <- nrnodes
        rf$forest$ndbigtree <- unlist(sapply(rflist, function(x) x$forest$ndbigtree))
        rf$forest$nodestatus <- do.call("cbind", lapply(rflist, 
            function(x) padm0(x$forest$nodestatus, nrnodes)))
        rf$forest$bestvar <- do.call("cbind", lapply(rflist, 
            function(x) padm0(x$forest$bestvar, nrnodes)))
        rf$forest$xbestsplit <- do.call("cbind", lapply(rflist, 
            function(x) padm0(x$forest$xbestsplit, nrnodes)))
        rf$forest$nodepred <- do.call("cbind", lapply(rflist, 
            function(x) padm0(x$forest$nodepred, nrnodes)))
        tree.dim <- dim(rf$forest$treemap)
        if (classRF) {
            rf$forest$treemap <- array(unlist(lapply(rflist, 
                function(x) apply(x$forest$treemap, 2:3, pad0, 
                  nrnodes))), c(nrnodes, 2, ntree))
        }
        else {
            rf$forest$leftDaughter <- do.call("cbind", lapply(rflist, 
                function(x) padm0(x$forest$leftDaughter, nrnodes)))
            rf$forest$rightDaughter <- do.call("cbind", lapply(rflist, 
                function(x) padm0(x$forest$rightDaughter, nrnodes)))
        }
        rf$forest$ntree <- ntree
        if (classRF) 
            rf$forest$cutoff <- rflist[[1]]$forest$cutoff
    }
    else {
        rf$forest <- NULL
    }
    #
    #Tons of stuff removed here...
    #
    if (classRF) {
        rf$confusion <- NULL
        rf$err.rate <- NULL
        if (haveTest) {
            rf$test$confusion <- NULL
            rf$err.rate <- NULL
        }
    }
    else {
        rf$mse <- rf$rsq <- NULL
        if (haveTest) 
            rf$test$mse <- rf$test$rsq <- NULL
    }
    rf
}
```

#* Results
```{r}


RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_FINAL_bal.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_FINAL_bal.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_FINAL_bal.rds'))

#**********************************************************************
# Variable Importance 
#**********************************************************************
library(data.table)
#varImpPlot(RF_All) 
varImpPlot(rfc1) 
varImpPlot(rfc2) 


# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])


# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 

View(RI3)

#**********************************************************************
# PREDICT ON FULL DATASET
#**********************************************************************

pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- cl_GW2$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = cl_GW2)
  n <- nrow(cl_GW2)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_full = mean(pcc) # 73.79461
sens_full = mean(sens) # 89.61538
spec_full = mean(spec) # 73.70403
gmean_full = sqrt(mean(sens)*mean(spec)) # 86.58883

#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}
# AUC For Full Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = cl_GW2, type='prob')
  AUC[i] = roc.area(cl_pred[,2], cl_GW2$Viol.Class) 
}
AUC_full = mean(AUC)

# Full Dataset
CV_full = data.frame(pcc_full = pcc_full,
                        sens_full = sens_full,
                        spec_full = spec_full,
                        gmean_full = gmean_full,
                        AUC_full = AUC_full)
                       
CV_full$pcc_full = round(CV_full$pcc_full,1)
CV_full$sens_full = round(CV_full$sens_full,1)
CV_full$spec_full = round(CV_full$spec_full,1)
CV_full$gmean_full = round(CV_full$gmean_full,1)
CV_full$AUC_full = round(CV_full$AUC_full,2)

View(CV_full)

saveRDS(RI3, file=paste0(out_dir, 'TopImp_RFC_GW_FinalModel.rds'))

```

#* Variable Importance Figure
```{r}


RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_FINAL_bal.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_FINAL_bal.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_FINAL_bal.rds'))
RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_FinalModel.rds'))


rf1 = RFC_model[[1]]
rf2 = RFC_model[[2]]
rf3 = RFC_model[[3]]
rf4 = RFC_model[[4]]
rf5 = RFC_model[[5]]
rf6 = RFC_model[[6]]
rf7 = RFC_model[[7]]
rf8 = RFC_model[[8]]
rf9 = RFC_model[[9]]
rf10 = RFC_model[[10]]

#RF_All = combine(rf1,rf2,rf3,rf4,rf5,rf6,rf7,rf8,rf9,rf10)
RF_All = my_combine(rf1,rf2,rf3,rf4,rf5,rf6,rf7,rf8,rf9,rf10)
varImpPlot(RF_All) 


#**********************************************************************
# Variable Importance Figure
#**********************************************************************

library(data.table)

# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])

# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 


#*******************************************************************
VarImpDF = RI3[1:10,]

VarImpDF = VarImpDF[order(VarImpDF$Relative_Importance),]

VarImpDF$Variable <- factor(VarImpDF$Variable, levels = as.character(VarImpDF$Variable))
VarImpDF$Variable.Short <- factor(VarImpDF$Variable.Short, levels = as.character(VarImpDF$Variable.Short))

# Colors go from bottom up (from rank 10 to rank 1)
# Blue = Climate/hydrology
# Green = Natural watershed/ geologic characteristics
# yellow = human land use
# Orange =  geologic
# red = N inputs
# purple = social

mypal = c('yellow','blue','red','red','red',
          'yellow','blue','yellow','yellow','purple')

ggplot(data=VarImpDF, aes(x=Variable.Short, y=Relative_Importance,fill=Variable.Short)) + 
  geom_bar(stat="identity")+
  labs(x=" ", y="Mean Decrease in Accuracy")+
  scale_fill_manual(values = mypal)+
  theme(axis.title=element_text(size=15))+
  theme(axis.text=element_text(size=15))+
  theme(panel.background = element_blank())+ # remove grey background
  theme(axis.line = element_line(colour = "black"))+ #+# add axis lines
  guides(fill=FALSE) + # removes legend
  coord_flip()



```

#*  Partial Dependence Plots
```{r}


RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_FINAL_bal.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_FINAL_bal.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_FINAL_bal.rds'))
RI3 <- readRDS(paste0(out_dir, 'TopImp_RFC_GW_FinalModel.rds'))

# Variable Definitions
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

library(plotmo)
library(pdp)
library(ggplot2)
library(gridExtra)



############################

rf1 = RFC_model[[1]]
rf2 = RFC_model[[2]]
rf3 = RFC_model[[3]]
rf4 = RFC_model[[4]]
rf5 = RFC_model[[5]]
rf6 = RFC_model[[6]]
rf7 = RFC_model[[7]]
rf8 = RFC_model[[8]]
rf9 = RFC_model[[9]]
rf10 = RFC_model[[10]]

RF_All = my_combine(rf1,rf2,rf3,rf4,rf5,rf6,rf7,rf8,rf9,rf10)
#RF_All = combine(rf1,rf2,rf3,rf4,rf5,rf6,rf7,rf8,rf9,rf10)

#model_fit = RF_model[[1]]
#model_fit = RF_model[[1]]
varImpTable = as.data.frame(RI)

# merge on short variable names
varImpTable = merge(varImpTable,varsunq,by.x='Variable',by.y='Variable.Name',all.x=T)
varImpTable <- varImpTable[order(-varImpTable$Relative_Importance),] 


ylabel = "Violation Class"

# Create training detaset for partial plots 
viol = TRAINERS[[1]] 

# put viol.class first
col_idx <- grep("Viol.Class", names(viol))
viol<- viol[, c(col_idx, (1:ncol(viol))[-col_idx])]
names(viol)


# Treatment Plot
treatrow = which(varImpTable$Variable=='TREATMENT')
treat1 <- partial(RF_All,train = viol,which.class=2,
                    pred.var="TREATMENT", chull= T)
treat.class = autoplot(treat1, contour = TRUE, ylab=ylabel,
                       xlab=paste0(varImpTable[treatrow,4]))+theme_bw()



temp1 <- partial(RF_All,train = viol,
                    pred.var="Perc_Hisp", chull= T)
pt.Hisp = autoplot(temp1, contour = TRUE, ylab=ylabel)+theme_bw()

#**********************
#  Variable 1
par.p1 <- partial(RF_All,train = viol,which.class=2,
                    pred.var=paste0(varImpTable[1,1]), chull= T)
p1<- autoplot(par.p1, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[1,4]))+theme_bw()

# Variable 2
par.p2 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[2,1]), chull= T)
p2<- autoplot(par.p2, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[2,4]))+theme_bw()

# Variable 3
par.p3 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[3,1]), chull= T)
p3<- autoplot(par.p3, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[3,4]))+theme_bw()

# Variable 4
par.p4 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[4,1]), chull= T)
p4<- autoplot(par.p4, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[4,4]))+theme_bw()

# Variable 5
par.p5 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[5,1]), chull= T)
p5<- autoplot(par.p5, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[5,4]))+theme_bw()

# Variable 6
par.p6 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[6,1]), chull= T)
p6<- autoplot(par.p6, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[6,4]))+theme_bw()

# Variable 7
par.p7 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[7,1]), chull= T)
p7<- autoplot(par.p7, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[7,4]))+theme_bw()

# Variable 8
par.p8 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[8,1]), chull= T)
p8<- autoplot(par.p8, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[8,4]))+theme_bw()

# Variable 9
par.p9 <- partial(RF_All, train = viol,which.class=2,
                  pred.var=paste0(varImpTable[9,1]), chull= T)
p9<- autoplot(par.p9, contour = TRUE, 
              ylab=ylabel,xlab=paste0(varImpTable[9,4]))+theme_bw()

# Variable 10
par.p10 <- partial(RF_All, train = viol,which.class=2,
                   pred.var=paste0(varImpTable[10,1]), chull= T)
p10<- autoplot(par.p10, contour = TRUE, 
               ylab=ylabel,xlab=paste0(varImpTable[10,4]))+theme_bw()

# Variable 11
# par.p11 <- partial(RF_All, train = viol,which.class=2, 
#                    pred.var=paste0(varImpTable[11,1]), chull= T)
# p11<- autoplot(par.p11, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 12
# par.p12 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[12,1]), chull= T)
# p12<- autoplot(par.p12, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 13
# par.p13 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[13,1]), chull= T)
# p13<- autoplot(par.p13, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 14
# par.p14 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[14,1]), chull= T)
# p14<- autoplot(par.p14, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 15
# par.p15 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[15,1]), chull= T)
# p15<- autoplot(par.p15, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 16
# par.p16 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[16,1]), chull= T)
# p16<- autoplot(par.p16, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 17
# par.p17 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[17,1]), chull= T)
# p17<- autoplot(par.p17, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 18
# par.p18 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[18,1]), chull= T)
# p18<- autoplot(par.p18, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 19
# par.p19 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[19,1]), chull= T)
# p19<- autoplot(par.p19, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 20
# par.p20 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[20,1]), chull= T)
# p20<- autoplot(par.p20, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 21
# par.p21 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[21,1]), chull= T)
# p21<- autoplot(par.p21, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 22
# par.p22 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[22,1]), chull= T)
# p22<- autoplot(par.p22, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 23
# par.p23 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[23,1]), chull= T)
# p23<- autoplot(par.p23, contour = TRUE, ylab=ylabel)+theme_bw()
# 
# # Variable 24
# par.p24 <- partial(RF_All, train = viol,which.class=2,
#                    pred.var=paste0(varImpTable[24,1]), chull= T)
# p24<- autoplot(par.p24, contour = TRUE, ylab=ylabel)+theme_bw()



# Plut Multip Partial Depce Plots

grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10, nrow=5,ncol=2)
#grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12, nrow=4,ncol=3)
#grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15, nrow=5,ncol=3)
#grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,
#             p10,p11,p12,p13,p14,p15,p16,p17,p18, nrow=6,ncol=3)
# grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,
#              p10,p11,p12,p13,p14,p15,p16,p17,p18,
#              p19,p20,p21,p22,p23,p24,nrow=6,ncol=4)

```

#**
# 93k Map of Predictions for 93K catchments
* Making predictions on the 93k catchments with PWSs
* 1. Use Python Script: "makeRasters300m.py", located in L:\Public\mpennino\Scripts
* 2. If necessary change: wi_names, inCSV, and OutRas
* 3. Save output to L:\Public\mpennino\SDWIS_Model
* 4. Open output in ArcMap: "RandomForest_Ouptput.mxd", located in "M:/Net MyDocuments\ArcGIS\SDWIS_Model
* 5. Export 
```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_FINAL_bal.rds'))

allPWS93 = cat.PWS.All.Vars2
allPWS65 = na.omit(allPWS93)

dim(allPWS93) # 93321   267
dim(allPWS65) # 65071   267

# Select out only catchments with GW systems
selectedRows  = (cat.PWS.All.Vars2$COMID %in% cat95in1_gw$COMID) # gives rows for all matching
allPWS1 = cat.PWS.All.Vars2[selectedRows,]

dim(cat95in1_gw) # 88083    1
dim(allPWS1) # 88057  267

allPWS = na.omit(allPWS1)

dim(allPWS) # 61161  266

library(randomForest)

rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
rfc3 = RFC_model[[3]]
rfc4 = RFC_model[[4]]
rfc5 = RFC_model[[5]]
rfc6 = RFC_model[[6]]
rfc7 = RFC_model[[7]]
rfc8 = RFC_model[[8]]
rfc9 = RFC_model[[9]]
rfc10 = RFC_model[[10]]

RF_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

start_time <- Sys.time()
pred_all = predict(RF_All,newdata = allPWS)
end_time <- Sys.time(); end_time - start_time
# Time difference of 1.704561 mins


# Convert Predicted output to dataframe
pred = as.data.frame(pred_all)

names(pred)[1] = "Pred.Viol.Class"

nrow(pred)  # 61159 
nrow(allPWS) # 61159    
  
# Merge COMID onto out_pred
# this adds COMID to Predicted Violations (assuming rows haven't changed)
pred$COMID = allPWS$COMID 
 
L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'
write.csv(pred, paste0(L_dir,"Model_Predictions/RFC_GW_pred_61k.csv"))

# % of Catchments predicted to have a violation
pred = read.csv(paste0(L_dir,"Model_Predictions/RFC_GW_pred_61k.csv"))
100*sum(pred$Pred.Viol.Class) / nrow(pred) # 21.37216%


```


# 2.6 Mill Map of FINAL Predictions for all 2.6 million catchments
* Making predictions on the 93k catchments with PWSs
* 1. Use Python Script: "makeRasters300m.py", located in L:\Public\ORDShare\NCEA\mpennino\Scripts
* 2. If necessary change: wi_names, inCSV, and OutRas
* 3. Save output to L:\Public\ORDShare\NCEA\mpennino\SDWIS_Model
* 4. Open output in ArcMap: "RandomForest_Ouptput.mxd", located in "M:/Net MyDocuments\ArcGIS\SDWIS_Model
* 5. Export 
```{r}
library(fst)
d_dir = 'C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Data/'

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_FINAL_bal.rds')) # using Balanced Model

all2.6cats = read_fst(paste0(d_dir,"All_Catchments_All_Variables.fst"))

library(randomForest)

rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
rfc3 = RFC_model[[3]]
rfc4 = RFC_model[[4]]
rfc5 = RFC_model[[5]]
rfc6 = RFC_model[[6]]
rfc7 = RFC_model[[7]]
rfc8 = RFC_model[[8]]
rfc9 = RFC_model[[9]]
rfc10 = RFC_model[[10]]

#RF_All = my_combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)
RF_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)


dim(all2.6cats) # 2647058     274

# Remove variables with excess NAs
rmov = c(# These have a lot of zeros
        'PctIce2011Cat','PctIce2011Ws',
        'S_TW2012Cat','S_TW2012Ws',
        'Pop.Served.avg','Pop.Served.km2',
        'MWST_2014','MAST_2014','MSST_2014'
        )
all2.6cats2 = all2.6cats[,!(names(all2.6cats) %in% rmov)]
rm(all2.6cats)
gc()
# Convert NA to 0 for TREATMENT
all2.6cats2$TREATMENT[is.na(all2.6cats2$TREATMENT)] <- 0

all2.6cats2 = na.omit(all2.6cats2)
dim(all2.6cats2) # 2487645     267


# Split up 2.6million catchments into 10 different groups
# append the 1 through 10 categories
sub = all2.6cats2
rm(all2.6cats2)
gc()
temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
temp2 = temp # **  Comment out if switching back to old method
temp2 = temp2[1:nrow(sub)] # cut off any extra rows
sub$CV_category = temp2 # Add the category to the dataframe
#sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe


allcatallvars1 = sub[sub$CV_category == 1,]
allcatallvars1 = subset(allcatallvars1, select=-c(CV_category))

allcatallvars2 = sub[sub$CV_category == 2,]
allcatallvars2 = subset(allcatallvars2, select=-c(CV_category))

allcatallvars3 = sub[sub$CV_category == 3,]
allcatallvars3 = subset(allcatallvars3, select=-c(CV_category))

allcatallvars4 = sub[sub$CV_category == 4,]
allcatallvars4 = subset(allcatallvars4, select=-c(CV_category))

allcatallvars5 = sub[sub$CV_category == 5,]
allcatallvars5 = subset(allcatallvars5, select=-c(CV_category))

allcatallvars6 = sub[sub$CV_category == 6,]
allcatallvars6 = subset(allcatallvars6, select=-c(CV_category))

allcatallvars7 = sub[sub$CV_category == 7,]
allcatallvars7 = subset(allcatallvars7, select=-c(CV_category))

allcatallvars8 = sub[sub$CV_category == 8,]
allcatallvars8 = subset(allcatallvars8, select=-c(CV_category))

allcatallvars9 = sub[sub$CV_category == 9,]
allcatallvars9 = subset(allcatallvars9, select=-c(CV_category))

allcatallvars10 = sub[sub$CV_category == 10,]
allcatallvars10 = subset(allcatallvars10, select=-c(CV_category))



start_time <- Sys.time()
predict1 = predict(RF_All,newdata = allcatallvars1)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.180673 mins

start_time <- Sys.time()
predict2 = predict(RF_All,newdata = allcatallvars2)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.4243 mins

start_time <- Sys.time()
predict3 = predict(RF_All,newdata = allcatallvars3)
end_time <- Sys.time(); end_time - start_time
# Time difference of 8.013974 mins

start_time <- Sys.time()
predict4 = predict(RF_All,newdata = allcatallvars4)
end_time <- Sys.time(); end_time - start_time
# Time difference of 7.919234 mins

start_time <- Sys.time()
predict5 = predict(RF_All,newdata = allcatallvars5)
end_time <- Sys.time(); end_time - start_time
# Time difference of 7.828754 mins

start_time <- Sys.time()
predict6 = predict(RF_All,newdata = allcatallvars6)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.803469 mins

start_time <- Sys.time()
predict7 = predict(RF_All,newdata = allcatallvars7)
end_time <- Sys.time(); end_time - start_time
# Time difference of 7.421586 mins

start_time <- Sys.time()
predict8 = predict(RF_All,newdata = allcatallvars8)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.676527 mins

start_time <- Sys.time()
predict9 = predict(RF_All,newdata = allcatallvars9)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.553773 mins

start_time <- Sys.time()
predict10 = predict(RF_All,newdata = allcatallvars10)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.271959 mins


# Convert Predicted output to dataframe
pred1 = as.data.frame(predict1)
pred2 = as.data.frame(predict2)
pred3 = as.data.frame(predict3)
pred4 = as.data.frame(predict4)
pred5 = as.data.frame(predict5)
pred6 = as.data.frame(predict6)
pred7 = as.data.frame(predict7)
pred8 = as.data.frame(predict8)
pred9 = as.data.frame(predict9)
pred10 = as.data.frame(predict10)

names(pred1)[1] = "Pred.Viol.Class"
names(pred2)[1] = "Pred.Viol.Class"
names(pred3)[1] = "Pred.Viol.Class"
names(pred4)[1] = "Pred.Viol.Class"
names(pred5)[1] = "Pred.Viol.Class"
names(pred6)[1] = "Pred.Viol.Class"
names(pred7)[1] = "Pred.Viol.Class"
names(pred8)[1] = "Pred.Viol.Class"
names(pred9)[1] = "Pred.Viol.Class"
names(pred10)[1] = "Pred.Viol.Class"

nrow(pred1)  # 248765 
nrow(allcatallvars1) # 248765    

nrow(pred3)  # 248765 
nrow(allcatallvars3) # 248765    

nrow(pred10)  # 248764 
nrow(allcatallvars10) # 248764    


# Merge COMID onto out_pred
# this adds COMID to Predicted Violations (assuming rows haven't changed)
pred1$COMID = allcatallvars1$COMID 
pred2$COMID = allcatallvars2$COMID 
pred3$COMID = allcatallvars3$COMID 
pred4$COMID = allcatallvars4$COMID 
pred5$COMID = allcatallvars5$COMID 
pred6$COMID = allcatallvars6$COMID 
pred7$COMID = allcatallvars7$COMID 
pred8$COMID = allcatallvars8$COMID 
pred9$COMID = allcatallvars9$COMID 
pred10$COMID = allcatallvars10$COMID 

pred_all = rbind(pred1,pred2,pred3,pred4,pred5,pred6,pred7,pred8,pred9,pred10)
nrow(pred_all) # 2487645
nrow(sub) # 2487645

D_dir = 'D:/Data/'
L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'
write.csv(pred_all, paste0(L_dir,"Model_Predictions/RFC_GW_pred_all_cats90.csv"))
# write_fst(pred_all, paste0(D_dir,"RFC_GW_pred_all_cats.fst"),
#           compress = 50, uniform_encoding = TRUE)

# % of Catchments predicted to have a violation
pred_all = read.csv(paste0(L_dir,"Model_Predictions/RFC_GW_pred_all_cats90.csv"))
100*sum(pred_all$Pred.Viol.Class) / nrow(pred_all) # 17.27666%


```

#***************************************
# 2.6 Mill Map of Bal Predictions for all 2.6 million catchments
* Making predictions on the 93k catchments with PWSs
* 1. Use Python Script: "makeRasters300m.py", located in L:\Public\ORDShare\NCEA\mpennino\Scripts
* 2. If necessary change: wi_names, inCSV, and OutRas
* 3. Save output to L:\Public\ORDShare\NCEA\mpennino\SDWIS_Model
* 4. Open output in ArcMap: "RandomForest_Ouptput.mxd", located in "M:/Net MyDocuments\ArcGIS\SDWIS_Model
* 5. Export 
```{r}
library(fst)
d_dir = 'C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Data/'

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_balanced80.rds'))

all2.6cats = read_fst(paste0(d_dir,"All_Catchments_All_Variables.fst"))

library(randomForest)

rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
rfc3 = RFC_model[[3]]
rfc4 = RFC_model[[4]]
rfc5 = RFC_model[[5]]
rfc6 = RFC_model[[6]]
rfc7 = RFC_model[[7]]
rfc8 = RFC_model[[8]]
rfc9 = RFC_model[[9]]
rfc10 = RFC_model[[10]]

RF_All = my_combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)
#RF_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)


dim(all2.6cats) # 2647058     274

# Remove variables with excess NAs
rmov = c(# These have a lot of zeros
        'PctIce2011Cat','PctIce2011Ws',
        'S_TW2012Cat','S_TW2012Ws',
        'Pop.Served.avg','Pop.Served.km2',
        'MWST_2014','MAST_2014','MSST_2014'
        )
all2.6cats2 = all2.6cats[,!(names(all2.6cats) %in% rmov)]

rm(all2.6cats)
gc()

# Convert NA to 0 for TREATMENT
all2.6cats2$TREATMENT[is.na(all2.6cats2$TREATMENT)] <- 0

all2.6cats2 = na.omit(all2.6cats2)
dim(all2.6cats2) # 2487645     267


# Split up 2.6million catchments into 10 different groups
# append the 1 through 10 categories
sub = all2.6cats2
rm(all2.6cat2)
gc()
temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
temp2 = temp # **  Comment out if switching back to old method
temp2 = temp2[1:nrow(sub)] # cut off any extra rows
sub$CV_category = temp2 # Add the category to the dataframe
#sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe


allcatallvars1 = sub[sub$CV_category == 1,]
allcatallvars1 = subset(allcatallvars1, select=-c(CV_category))

allcatallvars2 = sub[sub$CV_category == 2,]
allcatallvars2 = subset(allcatallvars2, select=-c(CV_category))

allcatallvars3 = sub[sub$CV_category == 3,]
allcatallvars3 = subset(allcatallvars3, select=-c(CV_category))

allcatallvars4 = sub[sub$CV_category == 4,]
allcatallvars4 = subset(allcatallvars4, select=-c(CV_category))

allcatallvars5 = sub[sub$CV_category == 5,]
allcatallvars5 = subset(allcatallvars5, select=-c(CV_category))

allcatallvars6 = sub[sub$CV_category == 6,]
allcatallvars6 = subset(allcatallvars6, select=-c(CV_category))

allcatallvars7 = sub[sub$CV_category == 7,]
allcatallvars7 = subset(allcatallvars7, select=-c(CV_category))

allcatallvars8 = sub[sub$CV_category == 8,]
allcatallvars8 = subset(allcatallvars8, select=-c(CV_category))

allcatallvars9 = sub[sub$CV_category == 9,]
allcatallvars9 = subset(allcatallvars9, select=-c(CV_category))

allcatallvars10 = sub[sub$CV_category == 10,]
allcatallvars10 = subset(allcatallvars10, select=-c(CV_category))




start_time <- Sys.time()
predict1 = predict(RF_All,newdata = allcatallvars1)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.180673 mins

start_time <- Sys.time()
predict2 = predict(RF_All,newdata = allcatallvars2)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.4243 mins

start_time <- Sys.time()
predict3 = predict(RF_All,newdata = allcatallvars3)
end_time <- Sys.time(); end_time - start_time
# Time difference of 8.013974 mins

start_time <- Sys.time()
predict4 = predict(RF_All,newdata = allcatallvars4)
end_time <- Sys.time(); end_time - start_time
# Time difference of 7.919234 mins

start_time <- Sys.time()
predict5 = predict(RF_All,newdata = allcatallvars5)
end_time <- Sys.time(); end_time - start_time
# Time difference of 7.828754 mins

start_time <- Sys.time()
predict6 = predict(RF_All,newdata = allcatallvars6)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.803469 mins

start_time <- Sys.time()
predict7 = predict(RF_All,newdata = allcatallvars7)
end_time <- Sys.time(); end_time - start_time
# Time difference of 7.421586 mins

start_time <- Sys.time()
predict8 = predict(RF_All,newdata = allcatallvars8)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.676527 mins

start_time <- Sys.time()
predict9 = predict(RF_All,newdata = allcatallvars9)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.553773 mins

start_time <- Sys.time()
predict10 = predict(RF_All,newdata = allcatallvars10)
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.271959 mins


# Convert Predicted output to dataframe
pred1 = as.data.frame(predict1)
pred2 = as.data.frame(predict2)
pred3 = as.data.frame(predict3)
pred4 = as.data.frame(predict4)
pred5 = as.data.frame(predict5)
pred6 = as.data.frame(predict6)
pred7 = as.data.frame(predict7)
pred8 = as.data.frame(predict8)
pred9 = as.data.frame(predict9)
pred10 = as.data.frame(predict10)

names(pred1)[1] = "Pred.Viol.Class"
names(pred2)[1] = "Pred.Viol.Class"
names(pred3)[1] = "Pred.Viol.Class"
names(pred4)[1] = "Pred.Viol.Class"
names(pred5)[1] = "Pred.Viol.Class"
names(pred6)[1] = "Pred.Viol.Class"
names(pred7)[1] = "Pred.Viol.Class"
names(pred8)[1] = "Pred.Viol.Class"
names(pred9)[1] = "Pred.Viol.Class"
names(pred10)[1] = "Pred.Viol.Class"

nrow(pred1)  # 248765 
nrow(allcatallvars1) # 248765    

nrow(pred3)  # 248765 
nrow(allcatallvars3) # 248765    

nrow(pred10)  # 248764 
nrow(allcatallvars10) # 248764    


# Merge COMID onto out_pred
# this adds COMID to Predicted Violations (assuming rows haven't changed)
pred1$COMID = allcatallvars1$COMID 
pred2$COMID = allcatallvars2$COMID 
pred3$COMID = allcatallvars3$COMID 
pred4$COMID = allcatallvars4$COMID 
pred5$COMID = allcatallvars5$COMID 
pred6$COMID = allcatallvars6$COMID 
pred7$COMID = allcatallvars7$COMID 
pred8$COMID = allcatallvars8$COMID 
pred9$COMID = allcatallvars9$COMID 
pred10$COMID = allcatallvars10$COMID 

pred_all = rbind(pred1,pred2,pred3,pred4,pred5,pred6,pred7,pred8,pred9,pred10)
nrow(pred_all) # 2487645
nrow(all2.6cats2) # 2487645

D_dir = 'D:/Data/'
L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'
write.csv(pred_all, paste0(L_dir,"Model_Predictions/RFC_Bal80_GW_pred_all_cats.csv"))
# write_fst(pred_all, paste0(D_dir,"RFC_Bal80_GW_pred_all_cats.fst"),
#           compress = 50, uniform_encoding = TRUE)

```

#***************************************

# RFC - CV - 80% Balanced
* This is same as above, except for the 10-fold cross validation, using a 80/20 split instead of standard 90/10
```{r}

set.seed(22)

# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_GW2[cl_GW2$Viol.Class == 0,] # separate cats without viols
  viol = cl_GW2[cl_GW2$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 80% training & 20% test datasets based on CV_category
  if(i < 10) { # for i = 1:9
    subtemp = sub[sub$CV_category != i & sub$CV_category != (i+1),]
    subtemp = subset(subtemp, select=-c(CV_category))
    } else {  # for i = 10
    subtemp = sub[sub$CV_category != i & sub$CV_category != 1,]
    subtemp = subset(subtemp, select=-c(CV_category))
    }
  if(i < 10) { # for i = 1:9
    subtest = sub[sub$CV_category == i | sub$CV_category == (i+1),]
    subtest = subset(subtest, select=-c(CV_category))
    } else {  # for i = 10
    subtest = sub[sub$CV_category == i | sub$CV_category == 1,]
    subtest = subset(subtest, select=-c(CV_category))
    }
  
  # select out the 90% training & 10% test datasets based on CV_category
  # subtemp = sub[sub$CV_category != i,]
  # subtemp = subset(subtemp, select=-c(CV_category))
  # subtest = sub[sub$CV_category == i,]
  # subtest = subset(subtest, select=-c(CV_category))

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 4.888982 mins





#**********************************************************************
# PREDICT ON FULL DATASET
#**********************************************************************

# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- cl_GW2$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = cl_GW2)
#   n <- nrow(cl_GW2)
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc)
# mean(sens) 
# mean(spec)
# sqrt(mean(sens)*mean(spec)) # 75.50967


#**********************************************************************
# Convert Predicted Probabilities to Actual Probabilities
#**********************************************************************
# ** USe this for predicting on unbalanced full or holdout datasets
p = beta * p_s / ((beta-1) * p_s + 1) 

# Where: 
# p_s = predicted probability
# beta =  ratio of the number majority class instances after undersampling over the number majority class ones in the original training set.


#**********************************************************************
# PREDICT ON FULL DATASET - Bias Corrected
#**********************************************************************

# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- cl_GW2$Viol.Class
#   beta = nrow(cl_GW2[cl_GW2$Viol.Class == 1,]) /
#     nrow(cl_GW2[cl_GW2$Viol.Class == 0,])
#   p_s_ = predict(RFC_model[[i]],newdata = cl_GW2)
#   p_s = as.numeric(as.character(p_s_))
#   ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
#   ypred = ifelse(ypred1 > 0.5,1,0)
#   n <- nrow(cl_GW2)
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc)
# mean(sens) # 96.15385
# mean(spec)
# sqrt(mean(sens)*mean(spec))

#**********************************************************************
# PREDICT ON TESTING DATASET
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc) #  75.57553
mean(sens,na.rm=T) # 74.21622
mean(spec) # 77.03784
sqrt(mean(sens)*mean(spec)) # 75.50967

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_balanced80.rds'))
saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_balanced80.rds'))
saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_balanced80.rds'))


```

#* Results80
```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_balanced80.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_balanced80.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_balanced80.rds'))

#**********************************************************************
rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]
# 
# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
library(data.table)
#varImpPlot(RF_All) 
varImpPlot(rfc1) 
varImpPlot(rfc2) 


# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])


# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 

View(RI3)



#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
#RFC_All  # Confusion matrix only works for the individual, not combined RF objects
rfc1
rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TRAINERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
#   n <- nrow(TRAINERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_train = mean(pcc)
# sens_train = mean(sens)
# spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

# For Test Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
  AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)


#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        gmean_test = gmean_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$gmean_test = round(CV_Results$gmean_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

View(CV_Results)

```

# RFC-CV-Bal-Hold80
```{r}


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# Make a 20% hold out dataset
set.seed(25)
cl_GW2$ROWID = seq.int(nrow(cl_GW2)) # add rowid
df_80 = cl_GW2[sample(nrow(cl_GW2), 0.8*nrow(cl_GW2)), ]
nrow(cl_GW2)
nrow(df_80)
nrow(cl_GW2[cl_GW2$Viol.Class>0,]) # 26
nrow(df_80[df_80$Viol.Class>0,]) #  13

selectedRows  = (!cl_GW2$ROWID %in% df_80$ROWID) # gives rows for all matching
df_20 = cl_GW2[selectedRows,]
nrow(df_20) # 2040
nrow(df_20[df_20$Viol.Class>0,]) #  13
cl_GW2 = subset(cl_GW2, select=-c(ROWID)) # remove ROWID
df_80 = subset(df_80, select=-c(ROWID)) # remove ROWID
df_20 = subset(df_20, select=-c(ROWID)) # remove ROWID

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


set.seed(22)


# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = df_80[df_80$Viol.Class == 0,] # separate cats without viols
  viol = df_80[df_80$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 80% training & 20% test datasets based on CV_category
  if(i < 10) { # for i = 1:9
    subtemp = sub[sub$CV_category != i & sub$CV_category != (i+1),]
    subtemp = subset(subtemp, select=-c(CV_category))
    } else {  # for i = 10
    subtemp = sub[sub$CV_category != i & sub$CV_category != 1,]
    subtemp = subset(subtemp, select=-c(CV_category))
    }
  if(i < 10) { # for i = 1:9
    subtest = sub[sub$CV_category == i | sub$CV_category == (i+1),]
    subtest = subset(subtest, select=-c(CV_category))
    } else {  # for i = 10
    subtest = sub[sub$CV_category == i | sub$CV_category == 1,]
    subtest = subset(subtest, select=-c(CV_category))
    }
  
  # select out the 90% training & 10% test datasets based on CV_category
  # subtemp = sub[sub$CV_category != i,]
  # subtemp = subset(subtemp, select=-c(CV_category))
  # subtest = sub[sub$CV_category == i,]
  # subtest = subset(subtest, select=-c(CV_category))

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = subtemp, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       strata = subtemp$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.621551 mins


#**********************************************************************
# PREDICT ON TESTING DATASET
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc) #  75.57553
mean(sens,na.rm=T) # 74.57576
mean(spec) # 76.56061
sqrt(mean(sens)*mean(spec))

#**********************************************************************
# PREDICT ON FULL DATASET
#**********************************************************************

# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- cl_GW2$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = cl_GW2)
#   n <- nrow(cl_GW2)
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# mean(pcc) # 77.16583
# mean(sens) # 89.11765
# mean(spec) # 77.0068
# sqrt(mean(sens)*mean(spec))

#**********************************************************************
# PREDICT ON HOLDOUT DATASET - bias Corrected
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc)
mean(sens)
mean(spec)
sqrt(mean(sens)*mean(spec))

saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_bal_hold80.rds'))
saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_bal_hold80.rds'))
saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_bal_hold80.rds'))
saveRDS(df_20, file=paste0(out_dir,'df_20_RFC_CV_GW_bal_hold80.rds'))

```

#* Results80
```{r}

RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_bal_hold80.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_bal_hold80.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_bal_hold80.rds'))
df_20 <- readRDS(paste0(out_dir, 'df_20_RFC_CV_GW_bal_hold80.rds'))

#**********************************************************************
rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]

# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
# library(data.table)
# #varImpPlot(RF_All) 
# varImpPlot(rfc1) 
# varImpPlot(rfc2) 
# 
# 
# # make a list of all variable importance tables
# imprtnce = list()
# for (i in 1:10){ 
#   imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
#   
#   names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
#   setDT(imprtnce[[i]], keep.rownames = TRUE)[]
#   imprtnce[[i]] = imprtnce[[i]][,1:2]
# }
# 
# # merge variables together
# imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
# imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')
# 
# names(imprtnce_all)[1] = 'Variable'
# 
# # Get average Relative Importance:
# imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])
# 
# # Merge on Short Variable Names
# varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')
# 
# 
# # Sort
# RI <- imprtnce_all[order(-Relative_Importance),] 
# RI = RI[,c('Variable','Relative_Importance')]
# 
# RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
#             by.x='Variable', by.y='Variable.Name', all.x=T)
# RI3 <- RI2[order(-Relative_Importance),] 
# 
# View(RI3)



#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
# RFC_All  # Confusion matrix only works for the individual, not combined RF objects
# rfc1
# rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TRAINERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
#   n <- nrow(TRAINERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_train = mean(pcc)
# sens_train = mean(sens)
# spec_train = mean(spec)

# TESTING DATASET
# pcc = c()
# sens = c()
# spec = c()
# for (i in 1:10){ 
#   yobs <- TESTERS[[i]]$Viol.Class
#   ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
#   n <- nrow(TESTERS[[i]])
#   pcc[i]  <- 100*sum(yobs == ypred) / n
#   sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
#   spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
# }
# pcc_test = mean(pcc)
# sens_test = mean(sens)
# spec_test = mean(spec)
# gmean_test = sqrt(sens_test*spec_test)

#**********************************************************************
# PREDICT ON HOLDOUT DATASET - bias Corrected
#**********************************************************************
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- df_20$Viol.Class
  beta = nrow(df_20[df_20$Viol.Class == 1,]) /
    nrow(df_20[df_20$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = df_20)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
    n <- nrow(df_20)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_hold = mean(pcc)
sens_hold = mean(sens)
spec_hold = mean(spec)
gmean_hold = sqrt(sens_hold*spec_hold)


#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


# For Test Dataset
# AUC = c()
# for (i in 1:10){ 
#   cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
#   AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
# }
# AUC_all = mean(AUC)


# For Holdout Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = df_20, type='prob')
  AUC[i] = roc.area(cl_pred[,2], df_20$Viol.Class) 
}
AUC_hold = mean(AUC)

#*******************************************************
# Make Table of Results
# CV_Results = data.frame(pcc_test = pcc_test,
#                         sens_test = sens_test,
#                         spec_test = spec_test,
#                         gmean_test = gmean_test,
#                         AUC_all = AUC_all)
#                        
# CV_Results$pcc_test = round(CV_Results$pcc_test,1)
# CV_Results$sens_test = round(CV_Results$sens_test,1)
# CV_Results$spec_test = round(CV_Results$spec_test,1)
# CV_Results$gmean_test = round(CV_Results$gmean_test,1)
# CV_Results$AUC_all = round(CV_Results$AUC_all,2)

# Holdout Results
CV_hold = data.frame(pcc_hold = pcc_hold,
                        sens_hold = sens_hold,
                        spec_hold = spec_hold,
                        gmean_hold = gmean_hold,
                        AUC_hold = AUC_hold)
                       
CV_hold$pcc_hold = round(CV_hold$pcc_hold,1)
CV_hold$sens_hold = round(CV_hold$sens_hold,1)
CV_hold$spec_hold = round(CV_hold$spec_hold,1)
CV_hold$gmean_hold = round(CV_hold$gmean_hold,1)
CV_hold$AUC_hold = round(CV_hold$AUC_hold,2)

#View(CV_Results)
View(CV_hold)

```

#********************************************

# New Cat Violations
* Calculate how many catchments without systems are predicted to be in violation
```{r}


D_dir = 'D:/Data/'
L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'
data_dir = 'C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Data/'

# Importing Data

# Predictions for 2.6 million Catchments
pred_all = read.csv(paste0(L_dir,"Model_Predictions/RFC_GW_pred_all_cats.csv"))

#pred_all = read.csv(paste0(L_dir,"Model_Predictions/RFC_FINAL_Bal_GW_pred_all_cats.csv"))
pred_all$Preds = as.numeric(as.character(pred_all$Pred.Viol.Class)) # 1153343


# All 93k catchments with PWSs that are found only in 1 catchment
cat.PWS.All.Vars = readRDS(paste0(data_dir,"ArcGIS/SDWIS_Model/RandomForest/Catchments_93K_PWS_All_Variables.rds"))
cat93 = as.data.frame(cat.PWS.All.Vars[,c('COMID')]); names(cat93)[1] = 'COMID'
cat93$hasSystem = 1 # labels all these COMIDs as having a system


# Merge
catsNO = merge(pred_all,cat93,by='COMID',all.x=T)

# Convert NA to 0
catsNO[is.na(catsNO)] <- 0

# Catchments predicted to be in violation, but without a PWS
catsNO$PrednoSys = ifelse(catsNO$Pred.Viol.Class == 1 & catsNO$hasSystem == 0,1,0)
sum(catsNO$PrednoSys) # 413306

# How many catchments predicted to be in violation? 
sum(catsNO$Preds) # 429782

# % of catchments in violation that are in catchments without PWS
100*sum(catsNO$PrednoSys) / sum(catsNO$Preds) # 96.16643%

# % of catchments Predicted to be in violation 
100*sum(catsNO$Preds) / nrow(catsNO) # 17.27666%

# Export to Make Map 

predYsysN = catsNO[,c('COMID','PrednoSys')]
names(predYsysN)[2] = 'Pred.Viol.Class'

D_dir = 'D:/Data/'
L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'

write.csv(predYsysN, paste0(L_dir,"Model_Predictions/RFC_GW_New_preds_all.csv"))
# write_fst(pred, paste0(D_dir,"RFC_GW_New_preds_all.fst"),
#          compress = 50, uniform_encoding = TRUE)



```


# Make map of ~25% of catchments that the RFC model did not correctly predict. 
* Identify how many catchments without violations are predicted to be in violation
```{r}

L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'

pred_bal = read.csv(paste0(L_dir,"Model_Predictions/RFC_GW_pred_61k.csv"))

dim(pred_bal) # 61159    3

# Prep Observed Data
obs = Cat.Viol.Freq.T.All.Vars.GW
obs$Viol.Class = ifelse(obs$Viol.Freq > 0,1,0)

obs = obs[,c('COMID','Viol.Class')]
dim(obs) # 5014    2

# Merge on Observed Violations
pred_obs = merge(pred_bal,obs,by='COMID',all.x=T)
pred_obs = pred_obs[,c('COMID','Pred.Viol.Class','Viol.Class')]

# Remove NAs
pred_obs = unique(pred_obs[complete.cases(pred_obs[,c("Viol.Class")]),])

# How many catchments without violations predicted to have violations
pred_obs$predYobsN = ifelse(pred_obs$Pred.Viol.Class == 1 & 
                              pred_obs$Viol.Class == 0,1,0)
sum(pred_obs$predYobsN) # 11559

# % of catchments without violations predicted to have violations
100 * sum(pred_obs$predYobsN) / nrow(pred_obs) # 20.24556


# How many catchments with violations predicted to NOT have violations
pred_obs$predNobsY = ifelse(pred_obs$Pred.Viol.Class == 0 & 
                              pred_obs$Viol.Class == 1,1,0)
sum(pred_obs$predNobsY) # 0

# % of catchments with violations predicted to NOT have violations
100 * sum(pred_obs$predNobsY) / nrow(pred_obs) # 0


# Export Catchments Not Correctly Predicted
pred_err = pred_obs[,c('COMID','predYobsN')]
names(pred_err)[2] = 'Pred.Viol.Class'

D_dir = 'D:/Data/'
L_dir = 'L:/Public/ORDShare/NCEA/mpennino/SDWIS_Model/'
write.csv(pred_err, paste0(L_dir,"Model_Predictions/RFC_GW_pred_error.csv"))

```

# Map of Predictions for modeled catchments used to build the model
* Making predictions on the 93k catchments with PWSs
* 1. Use Python Script: "makeRasters300m.py", located in L:\Public\mpennino\Scripts
* 2. If necessary change: wi_names, inCSV, and OutRas
* 3. Save output to L:\Public\mpennino\SDWIS_Model
* 4. Open output in ArcMap: "RandomForest_Ouptput.mxd", located in "M:/Net MyDocuments\ArcGIS\SDWIS_Model
* 5. Export 
```{r}

# Gets model input with NAs removed and with COMID
viol_comid = na.omit(Cat.Viol.Freq.T.All.Vars.GW2)
viol_comid2 = subset(viol_comid, select = -c(Viol.Freq))

# Gets model input without response variable
percviol3 = subset(cl_GW2, select = -c(Viol.Class))


# PREDICTION
#predict_5k <- predict(percviol_out_rf1, newdata = percviol3) # with NAs
i=1
predict_m = predict(RFC_model[[i]],newdata = percviol3)

# Convert Predicted output to dataframe
predict_m = as.data.frame(predict_m)
names(predict_m)[1] = "Pred.Viol.Class"

nrow(cl_GW2) # 60862 
nrow(predict_m) # 60862 
nrow(viol_comid2) # 60862    

# Merge COMID onto out_pred
predict_m$COMID = viol_comid2$COMID # this adds COMID to Predicted Violations (assuming rows haven't changed)



#write.csv(predict_m, file= 'L:/Public/mpennino/SDWIS_Model/RF_pred_viol_Class_GW_modeled.csv')



```


# Map of Residuals
* Making predictions on the 93k catchments with PWSs
* 1. Use Python Script: "makeRasters300m.py", located in L:\Public\mpennino\Scripts
* 2. If necessary change: wi_names, inCSV, and OutRas
* 3. Save output to L:\Public\...
* 4. Open output in ArcMap: "RandomForest_Ouptput.mxd", located in
* 5. Export 
```{r}

# Gets observed perc_viol and COMID without NA rows
viol_obs = viol_comid[,1:2]
viol_obs$Viol.Class = ifelse(viol_obs$Viol.Freq > 0,1,0)


nrow(viol_obs) # 60862
nrow(predict_m) # 60862

# Merge on Observed values
pred_resid = merge(predict_m,viol_obs,by='COMID')

pred_resid$Pred.Viol.Class = as.numeric(pred_resid$Pred.Viol.Class)

pred_resid$resid = pred_resid$Viol.Class - pred_resid$Pred.Viol.Class

#write.csv(pred_resid, file= 'L:/Public/mpennino/SDWIS_Model/RF_pred_Viol_Class_GW_modeled_Residuals.csv')


```

# R Map
* The ArcGIS map was made after saving CSV in "SDWIS_Model_Variables.Rmd" and then uploading in ArcGIS and joing to catchments shapefile.  
```{r}
library(rgdal)
library(rgeos)
library(foreign)

states = readOGR('M:/Net MyDocuments/ArcGIS/Boundaries/US Census States/FACTFINDER/Census_Conterminous_States_USGS.shp')

# Observed Violation Cats
# Cat_93K_ViolFreq = readOGR('M:/Net MyDocuments/ArcGIS/SDWIS_Model/SDWIS/Spatial/Catchments_Observed_93K_Viol_Freq2.shp')
# Cat_5K_ViolFreq = readOGR('M:/Net MyDocuments/ArcGIS/SDWIS_Model/SDWIS/Spatial/Catchments_Observed_Viol_Freq2.shp')

# This only has catchments with violations 

Obs_Viol_Cats = readOGR('M:/Net MyDocuments/ArcGIS/SDWIS_Model/Maps/Observed/Map_Catchments_w_GW_Viol_2013to17_Viols.shp')

Obs_ViolnoViol_Cats = readOGR('M:/Net MyDocuments/ArcGIS/SDWIS_Model/Maps/Observed/Map_Catchments_w_GW_Viol_2013to17_both.shp')


proj4string(states)
proj4string(Obs_Viol_Cats)
proj4string(Obs_ViolnoViol_Cats)

#proj4string(Cat_5K_ViolFreq)
#proj4string(Cat_93K_ViolFreq)

Obs_Viol_Cats <- spTransform(Obs_Viol_Cats,CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
Obs_ViolnoViol_Cats <- spTransform(Obs_ViolnoViol_Cats,CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Cat_93K_ViolFreq <- spTransform(Cat_93K_ViolFreq,CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
# Cat_5K_ViolFreq <- spTransform(Cat_5K_ViolFreq,CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))

#Subset out Catagories into separate dataframes
# Add category of 1 or 0 for violation or not
Obs_Viol_Cats$perc_viol = as.numeric(as.character(Obs_Viol_Cats$perc_viol))
Obs_ViolnoViol_Cats$perc_viol =
  as.numeric(as.character(Obs_ViolnoViol_Cats$perc_viol))

Obs_Viol_Cats_noViol = Obs_ViolnoViol_Cats[Obs_ViolnoViol_Cats$perc_viol == 0,]
Obs_Viol_Cats_Viol = Obs_ViolnoViol_Cats[Obs_ViolnoViol_Cats$perc_viol > 0,]

# Cat_5K_ViolFreq$Viol_Freq = as.numeric(as.character(Cat_5K_ViolFreq$Viol_Freq))
# Cat_5K_ViolFreq_noViol = Cat_5K_ViolFreq[Cat_5K_ViolFreq$Viol_Freq == 0,]
# Cat_5K_ViolFreq_Viol = Cat_5K_ViolFreq[Cat_5K_ViolFreq$Viol_Freq > 0,]
# 
# Cat_93K_ViolFreq_noViol = Cat_93K_ViolFreq[Cat_93K_ViolFreq$Viol_Freq2 == 1,]
# Cat_93K_ViolFreq_noNViol = Cat_93K_ViolFreq[Cat_93K_ViolFreq$Viol_Freq2 == 2,]
# Cat_93K_ViolFreq_Viol = Cat_93K_ViolFreq[Cat_93K_ViolFreq$Viol_Freq2 == 3,]

# Plot for catchments with and without violations
# plot(states)
# plot(Cat_5K_ViolFreq_noViol,border='blue',add=T)
# plot(Cat_5K_ViolFreq_Viol,border='red',add=T)

# Plot for all 93k catchments, inventory of no viol, no NO3 viol and NO3 violators
#start_time <- Sys.time()
#plot(states)
#plot(Cat_93K_ViolFreq_noViol,border='grey',add=T)
#plot(Cat_93K_ViolFreq_noNViol,border='blue',add=T)
#plot(Cat_93K_ViolFreq_Viol,border='red',add=T)
#end_time <- Sys.time(); end_time - start_time
# Time difference of 34.3951 mins

# Plot for all Observed catchments, inventory of no viol, no NO3 viol and NO3 violators
start_time <- Sys.time()
plot(states)
#plot(Obs_Viol_Cats_noViol,border='grey',add=T)
#plot(Obs_Viol_Cats_noViol,border='blue',add=T)
plot(Obs_Viol_Cats_Viol,border='red',add=T)
end_time <- Sys.time(); end_time - start_time
# Time difference of 1.46661 mins


```

#********************************************


# Tesging out a Feature Slection Technique
# RFC - CV - FS
```{r}
set.seed(22)
library(mRMRe)

# Number of Features to Select (FEATURE COUNT)
FC = 80
# 10, gmean=74.21214
# 20, gmean=75.60042
# 30, gmean=74.2421
# 40, gmean=76.6769


new_df = cl_GW2
#new_df$Viol.Class = as.numeric(new_df$Viol.Class)

# Run the Model
RFC_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Feature Selection
  fs_df <- mRMR.data(data = subtemp)
  out_c = mRMR.classic(data = fs_df, 
                     target_indices= ncol(subtemp), # Index of the target
                     feature_count = FC) # Number of features to be selected 
  fs_ind = unlist(solutions(out_c))
  fs_train = subtemp[,c(ncol(subtemp),fs_ind)]
  fs_test = subtest[,c(ncol(subtemp),fs_ind)]

  TRAINERS[[i]] = fs_train
  TESTERS[[i]] = fs_test

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  RFC_model[[i]] <- randomForest(as.factor(Viol.Class) ~ ., 
                       data = fs_train, 
                       importance=T, 
                       ntree =1000,  # number of Trees
                       #strata = fs_train$Viol.Class,
                       sampsize = rep(cmin_,2), # Balancing
                       replace=TRUE)  
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.197315 mins

# Test Dataset
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc)
mean(sens)
mean(spec)
sqrt(mean(sens)*mean(spec))

# saveRDS(RFC_model, file=paste0(out_dir, 'RFC_model_CV_GW_FS.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_RFC_CV_GW_FS.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_RFC_CV_GW_FS.rds'))

```

#* Results
```{r}


RFC_model <- readRDS(paste0(out_dir, 'RFC_model_CV_GW_FS.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_RFC_CV_GW_FS.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_RFC_CV_GW_FS.rds'))


rfc1 = RFC_model[[1]]
rfc2 = RFC_model[[2]]
# rfc3 = RFC_model[[3]]
# rfc4 = RFC_model[[4]]
# rfc5 = RFC_model[[5]]
# rfc6 = RFC_model[[6]]
# rfc7 = RFC_model[[7]]
# rfc8 = RFC_model[[8]]
# rfc9 = RFC_model[[9]]
# rfc10 = RFC_model[[10]]
# 
# RFC_All = combine(rfc1,rfc2,rfc3,rfc4,rfc5,rfc6,rfc7,rfc8,rfc9,rfc10)

# 3,4,5,7,8,9
#**********************************************************************
# Variable Importance 
#**********************************************************************
library(data.table)
#varImpPlot(RF_All) 
varImpPlot(rfc1) 
varImpPlot(rfc2) 


# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){ 
  imprtnce[[i]] = as.data.frame(importance(RFC_model[[i]]))
  names(imprtnce[[i]])[1] = paste0("%IncMSE_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]
  imprtnce[[i]] = imprtnce[[i]][,1:2]
}

# Add in variables not selected and give them a zero importance
for (i in 1:10){
  var_names = as.data.frame(names(cl_GW2))
  colnames(var_names)[1] = "rn"
  var_names[,2] = 0
  names(var_names)[2] = paste0("%IncMSE_",i) # give variable name
  selectedRows  = (!var_names$rn %in% imprtnce[[i]]$rn) # rows for not matching
  temp = var_names[selectedRows,]
  imprtnce[[i]] = rbind(imprtnce[[i]],temp)
}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])

# Merge on Short Variable Names
varsunq = read.csv('C:/Users/MPennino/OneDrive - Environmental Protection Agency (EPA)/Projects/StreamCat/VariableDefs_SDWIS.csv')

RI2 = merge(RI,varsunq[c('Variable.Name','Variable.Short')], 
            by.x='Variable', by.y='Variable.Name', all.x=T)
RI3 <- RI2[order(-Relative_Importance),] 

View(RI3)



#**********************************************************************
# Produce Table of OOB estimate of  error rate and Confusion matrix:
#**********************************************************************
RFC_All  # Confusion matrix only works for the individual, not combined RF objects
rfc1
rfc2

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TRAINERS[[i]])
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred = predict(RFC_model[[i]],newdata = TESTERS[[i]])
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#**********************************************************************
# PREDICT ON FULL DATASET - Bias Corrected
#**********************************************************************

pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- cl_GW2$Viol.Class
  beta = nrow(cl_GW2[cl_GW2$Viol.Class == 1,]) /
    nrow(cl_GW2[cl_GW2$Viol.Class == 0,])
  p_s_ = predict(RFC_model[[i]],newdata = cl_GW2)
  p_s = as.numeric(as.character(p_s_))
  ypred1 = beta * p_s / ((beta-1) * p_s + 1) 
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(cl_GW2)
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_full = mean(pcc)
sens_full = mean(sens)
spec_full = mean(spec)
gmean_full = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

# For Test Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = TESTERS[[i]], type='prob')
  AUC[i] = roc.area(cl_pred[,2], TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

# For Full Dataset
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(RFC_model[[i]], newdata = cl_GW2, type='prob')
  AUC[i] = roc.area(cl_pred[,2], cl_GW2$Viol.Class) 
}
AUC_full = mean(AUC)

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        gmean_test = gmean_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$gmean_test = round(CV_Results$gmean_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

# Full Dataset
CV_full = data.frame(pcc_full = pcc_full,
                        sens_full = sens_full,
                        spec_full = spec_full,
                        gmean_full = gmean_full,
                        AUC_full = AUC_full)
                       
CV_full$pcc_full = round(CV_full$pcc_full,1)
CV_full$sens_full = round(CV_full$sens_full,1)
CV_full$spec_full = round(CV_full$spec_full,1)
CV_full$gmean_full = round(CV_full$gmean_full,1)
CV_full$AUC_full = round(CV_full$AUC_full,2)

View(CV_Results)
View(CV_full)


```


#**



#******************************************

# The following code use used to make comparisons with other classification models such as
* Logistic regression
* Boosted Regression (BRT)
* Support Vector Machines (SVM)
* Artificial Neural Netowrks (ANN)
* Bayesian Netowrks (BN)
* Generalized Addative Models (GAM)

#********************************************
# Logit-CV-FULL
```{r}

set.seed(100)

new_df = cl_GW2

# Run the Model
logit_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next lines of code are for balancing 
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  sub = rbind(viol,noviol) # Combine the data back together

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Get sample size to be same for each CV run  
  # subtemp = subtemp[sample(nrow(subtemp), cmin*2), ] # randomly sample rows
  # subtest = subtest[sample(nrow(subtest), cmin_t*2), ] # randomly sample rows

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  logit_model[[i]] <- glm(Viol.Class ~.,
                          family=binomial(link='logit'),
                          data=subtemp) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 15.47934 mins
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 

# saveRDS(logit_model, file=paste0(out_dir, 'logit_model_CV_GW_full.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_logit_CV_GW_full.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_logit_CV_GW_full.rds'))

logit_model <- readRDS(paste0(out_dir, 'logit_model_CV_GW_full.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_logit_CV_GW_full.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_logit_CV_GW_full.rds'))

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TRAINERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TESTERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(logit_model[[i]],
                    newdata=subset(TESTERS[[i]],
                                   select=-c(Viol.Class)),
                    type='response')
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# Logit - CV - Balanced
```{r}

set.seed(100)


# Run the Model
logit_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_GW2[cl_GW2$Viol.Class == 0,] # separate cats without viols
  viol = cl_GW2[cl_GW2$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Get sample size to be same for each CV run  
  # subtemp = subtemp[sample(nrow(subtemp), cmin*2), ] # randomly sample rows
  # subtest = subtest[sample(nrow(subtest), cmin_t*2), ] # randomly sample rows

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  logit_model[[i]] <- glm(Viol.Class ~.,
                          family=binomial(link='logit'),
                          data=subtemp) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 17.37344 secs
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 

# saveRDS(logit_model, file=paste0(out_dir, 'logit_model_CV_GW_balanced.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_logit_CV_GW_balanced.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_logit_CV_GW_balanced.rds'))

logit_model <- readRDS(paste0(out_dir, 'logit_model_CV_GW_balanced.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_logit_CV_GW_balanced.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_logit_CV_GW_balanced.rds'))

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TRAINERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TESTERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(logit_model[[i]],
                    newdata=subset(TESTERS[[i]],
                                   select=-c(Viol.Class)),
                    type='response')
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# Logit - CV - Hypothesized
```{r}


# Tyring with Hypothesized variables
new_df = cl_GW2[,hypo_vars]

set.seed(100)
# Run the Model
logit_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Get sample size to be same for each CV run  
  # subtemp = subtemp[sample(nrow(subtemp), cmin*2), ] # randomly sample rows
  # subtest = subtest[sample(nrow(subtest), cmin_t*2), ] # randomly sample rows

  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest

  # Run the model
  cmin_ <- min(table(subtemp$Viol.Class))  
  logit_model[[i]] <- glm(Viol.Class ~.,
                          family=binomial(link='logit'),
                          data=subtemp) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.196916 secs
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 

# saveRDS(logit_model, file=paste0(out_dir, 'logit_model_CV_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_logit_CV_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_logit_CV_GW_hypoth.rds'))

# logit_model <- readRDS(paste0(out_dir, 'logit_model_CV_GW_hypoth.rds'))
# TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_logit_CV_GW_hypoth.rds'))
# TESTERS <- readRDS(paste0(out_dir, 'TESTERS_logit_CV_GW_hypoth.rds'))

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TRAINERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TESTERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(logit_model[[i]],
                    newdata=subset(TESTERS[[i]],
                                   select=-c(Viol.Class)),
                    type='response')
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# Logit - CV - FS
```{r}

library(mRMRe)

# Number of Features to Select (FEATURE COUNT)
FC = 109
# 10, sens=69.92252
# 20, sens=69.92252
# 30, sens=
# 40, sens=71.39099
# 50, sens=
# 60, sens=
# 80, sens=72.6


new_df = cl_GW2

set.seed(100)
# Run the Model
logit_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Get sample size to be same for each CV run  
  # subtemp = subtemp[sample(nrow(subtemp), cmin*2), ] # randomly sample rows
  # subtest = subtest[sample(nrow(subtest), cmin_t*2), ] # randomly sample rows

  # Feature Selection
  fs_df <- mRMR.data(data = subtemp)
  out_c = mRMR.classic(data = fs_df, 
                     target_indices= ncol(subtemp), # Index of the target
                     feature_count = FC) # Number of features to be selected 
  fs_ind = unlist(solutions(out_c))
  fs_train = subtemp[,c(ncol(subtemp),fs_ind)]
  fs_test = subtest[,c(ncol(subtemp),fs_ind)]

  TRAINERS[[i]] = fs_train
  TESTERS[[i]] = fs_test

  # Run the model
  cmin_ <- min(table(fs_train$Viol.Class))  
  logit_model[[i]] <- glm(Viol.Class ~.,
                          family=binomial(link='logit'),
                          data=fs_train) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.196916 secs
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 



pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TESTERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc)
mean(sens)
mean(spec)


```

# Results
```{r}
# saveRDS(logit_model, file=paste0(out_dir, 'logit_model_CV_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_logit_CV_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_logit_CV_GW_hypoth.rds'))

# logit_model <- readRDS(paste0(out_dir, 'logit_model_CV_GW_hypoth.rds'))
# TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_logit_CV_GW_hypoth.rds'))
# TESTERS <- readRDS(paste0(out_dir, 'TESTERS_logit_CV_GW_hypoth.rds'))

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TRAINERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(logit_model[[i]],
                  newdata=subset(TESTERS[[i]],select=-c(Viol.Class)),
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(logit_model[[i]],
                    newdata=subset(TESTERS[[i]],
                                   select=-c(Viol.Class)),
                    type='response')
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)
```

# SVM-CV-FULL
```{r}
library("e1071")

new_df = cl_GW2

set.seed(22)

# Run the Model
svm_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next lines of code are for balancing 
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  sub = rbind(viol,noviol) # Combine the data back together

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  svm_model[[i]] <- svm(Viol.Class ~ ., data=subtemp)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 22.62318 secs


# saveRDS(svm_model, file=paste0(out_dir, 'svm_model_CV_Class_GW_balanced.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_svm_CV_Class_GW_balanced.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_svm_CV_Class_GW_balanced.rds'))

svm_model <- readRDS(paste0(out_dir, 'svm_model_CV_Class_GW_balanced.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_svm_CV_Class_GW_balanced.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_svm_CV_Class_GW_balanced.rds'))



#*************************************************************************
# Variable Importance
# see: https://stackoverflow.com/questions/34781495/how-to-find-important-factors-in-support-vector-machine
#*************************************************************************
library(data.table)
w <- t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV   # weight vectors
w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight
w <- sort(w, decreasing = T)
print(w)

# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){
  imprtnce[[i]] <- 
    t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV # weight vectors
  imprtnce[[i]] <- 
    apply(imprtnce[[i]], 2, function(v){sqrt(sum(v^2))})  # weight
  imprtnce[[i]] <- as.data.frame(sort(w, decreasing = T))
  names(imprtnce[[i]])[1] = paste0("RI_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]

}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]



#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TRAINERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(svm_model[[i]],newdata = TESTERS[[i]])
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# SVM - CV - Balanced
```{r}
library("e1071")

set.seed(22)

# Run the Model
svm_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_GW2[cl_GW2$Viol.Class == 0,] # separate cats without viols
  viol = cl_GW2[cl_GW2$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  svm_model[[i]] <- svm(Viol.Class ~ ., data=subtemp)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 22.62318 secs


# saveRDS(svm_model, file=paste0(out_dir, 'svm_model_CV_Class_GW_balanced.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_svm_CV_Class_GW_balanced.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_svm_CV_Class_GW_balanced.rds'))

svm_model <- readRDS(paste0(out_dir, 'svm_model_CV_Class_GW_balanced.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_svm_CV_Class_GW_balanced.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_svm_CV_Class_GW_balanced.rds'))



#*************************************************************************
# Variable Importance
# see: https://stackoverflow.com/questions/34781495/how-to-find-important-factors-in-support-vector-machine
#*************************************************************************
library(data.table)
w <- t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV   # weight vectors
w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight
w <- sort(w, decreasing = T)
print(w)

# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){
  imprtnce[[i]] <- 
    t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV # weight vectors
  imprtnce[[i]] <- 
    apply(imprtnce[[i]], 2, function(v){sqrt(sum(v^2))})  # weight
  imprtnce[[i]] <- as.data.frame(sort(w, decreasing = T))
  names(imprtnce[[i]])[1] = paste0("RI_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]

}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]



#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TRAINERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(svm_model[[i]],newdata = TESTERS[[i]])
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# SVM - CV - Hypoth
```{r}
library("e1071")

# Tyring with Hypothesized variables
new_df = cl_GW2[,hypo_vars]

set.seed(22)

# Run the Model
svm_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  svm_model[[i]] <- svm(Viol.Class ~ ., data=subtemp)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 9.110317 secs


# saveRDS(svm_model, file=paste0(out_dir, 'svm_model_CV_Class_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_svm_CV_Class_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_svm_CV_Class_GW_hypoth.rds'))

# svm_model <- readRDS(paste0(out_dir, 'svm_model_CV_Class_GW_hypoth.rds'))
# TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_svm_CV_Class_GW_hypoth.rds'))
# TESTERS <- readRDS(paste0(out_dir, 'TESTERS_svm_CV_Class_GW_hypoth.rds'))



#*************************************************************************
# Variable Importance
# see: https://stackoverflow.com/questions/34781495/how-to-find-important-factors-in-support-vector-machine
#*************************************************************************
library(data.table)
w <- t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV   # weight vectors
w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight
w <- sort(w, decreasing = T)
print(w)

# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){
  imprtnce[[i]] <- 
    t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV # weight vectors
  imprtnce[[i]] <- 
    apply(imprtnce[[i]], 2, function(v){sqrt(sum(v^2))})  # weight
  imprtnce[[i]] <- as.data.frame(sort(w, decreasing = T))
  names(imprtnce[[i]])[1] = paste0("RI_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]

}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]



#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TRAINERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(svm_model[[i]],newdata = TESTERS[[i]])
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# SVM - CV - FS
```{r}
library("e1071")

library(mRMRe)

# Number of Features to Select (FEATURE COUNT)
FC = 18
# 10, sens=71.25586
# 15, sens=73.26126
# 16, sens=73.53153
# 17, sens=73.26667
# 18, sens=74.06667
# 19, sens=73.8018
# 20, sens=73.0036
# 21, sens=
# 12, sens=
# 50, sens=71.53153
# 80, sens=69.52252
# 100, sens=69.64865
# 120, sens=69.92432

# Tyring with Hypothesized variables
new_df = cl_GW2

set.seed(22)

# Run the Model
svm_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Feature Selection
  fs_df <- mRMR.data(data = subtemp)
  out_c = mRMR.classic(data = fs_df, 
                     target_indices= ncol(subtemp), # Index of the target
                     feature_count = FC) # Number of features to be selected 
  fs_ind = unlist(solutions(out_c))
  fs_train = subtemp[,c(ncol(subtemp),fs_ind)]
  fs_test = subtest[,c(ncol(subtemp),fs_ind)]

  TRAINERS[[i]] = fs_train
  TESTERS[[i]] = fs_test
  
  # Run the model
  svm_model[[i]] <- svm(Viol.Class ~ ., data=fs_train)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 9.110317 secs


pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc)
mean(sens)
mean(spec)



```

# Results
```{r}
# saveRDS(svm_model, file=paste0(out_dir, 'svm_model_CV_Class_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_svm_CV_Class_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_svm_CV_Class_GW_hypoth.rds'))

# svm_model <- readRDS(paste0(out_dir, 'svm_model_CV_Class_GW_hypoth.rds'))
# TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_svm_CV_Class_GW_hypoth.rds'))
# TESTERS <- readRDS(paste0(out_dir, 'TESTERS_svm_CV_Class_GW_hypoth.rds'))



#*************************************************************************
# Variable Importance
# see: https://stackoverflow.com/questions/34781495/how-to-find-important-factors-in-support-vector-machine
#*************************************************************************
library(data.table)
w <- t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV   # weight vectors
w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight
w <- sort(w, decreasing = T)
print(w)

# make a list of all variable importance tables
imprtnce = list()
for (i in 1:10){
  imprtnce[[i]] <- 
    t(svm_model[[i]]$coefs) %*% svm_model[[i]]$SV # weight vectors
  imprtnce[[i]] <- 
    apply(imprtnce[[i]], 2, function(v){sqrt(sum(v^2))})  # weight
  imprtnce[[i]] <- as.data.frame(sort(w, decreasing = T))
  names(imprtnce[[i]])[1] = paste0("RI_",i) # give variable name
  setDT(imprtnce[[i]], keep.rownames = TRUE)[]

}

# merge variables together
imprtnce_all = merge(imprtnce[[1]],imprtnce[[2]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[3]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[4]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[5]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[6]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[7]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[8]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[9]],by='rn')
imprtnce_all = merge(imprtnce_all,imprtnce[[10]],by='rn')

names(imprtnce_all)[1] = 'Variable'

# Get average Relative Importance:
imprtnce_all$Relative_Importance = rowMeans(imprtnce_all[,2:11])

# Sort
RI <- imprtnce_all[order(-Relative_Importance),] 
RI = RI[,c('Variable','Relative_Importance')]



#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TRAINERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(svm_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){ 
  cl_pred = predict(svm_model[[i]],newdata = TESTERS[[i]])
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)
```


# BRT - CV - Balanced
```{r}
library(gbm)


set.seed(22)
NTREES = 1000

# Run the Model
brt_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_GW2[cl_GW2$Viol.Class == 0,] # separate cats without viols
  viol = cl_GW2[cl_GW2$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  brt_model[[i]] = gbm(formula = Viol.Class ~ .,
               distribution = "gaussian",
               data = subtemp,
               n.trees = NTREES,
               interaction.depth=1,
               shrinkage = .05, # initially .01
               n.minobsinnode = 75) # initially 20
  }
end_time <- Sys.time(); end_time - start_time
# Time difference of 48.1347 secs, with n.trees = 1000

# saveRDS(brt_model, file=paste0(out_dir,'brt_model_CV_class_GW_balanced.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_brt_CV_class_GW_balanced.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_brt_CV_class_GW_balanced.rds'))


brt_model <- readRDS(paste0(out_dir, 'brt_model_CV_class_GW_balanced.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_brt_CV_class_GW_balanced.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_brt_CV_class_GW_balanced.rds'))


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TRAINERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){
  cl_pred = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')

  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# BRT - CV - Hypoth
```{r}
library(gbm)

# Tyring with Hypothesized variables
new_df = cl_GW2[,hypo_vars]

set.seed(22)
NTREES = 1000

# Run the Model
brt_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  brt_model[[i]] = gbm(formula = Viol.Class ~ .,
               distribution = "gaussian",
               data = subtemp,
               n.trees = NTREES,
               interaction.depth=1,
               shrinkage = .05, # initially .01
               n.minobsinnode = 75) # initially 20
  }
end_time <- Sys.time(); end_time - start_time
# Time difference of 17.50918 secs, with n.trees = 1000

# saveRDS(brt_model, file=paste0(out_dir,'brt_model_CV_class_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_brt_CV_class_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_brt_CV_class_GW_hypoth.rds'))

brt_model <- readRDS(paste0(out_dir, 'brt_model_CV_class_GW_hypoth.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_brt_CV_class_GW_hypoth.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_brt_CV_class_GW_hypoth.rds'))


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TRAINERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){
  cl_pred = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')

  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)



```

# BRT - CV - FS
```{r}
library(gbm)

library(mRMRe)

NTREES = 10000
NODES = 10
shrk = .01
intdpth = 2

# Number of Features to Select (FEATURE COUNT)
FC = 10 #(NODES/#)
# 10, sens=9/73.25225,10/73.78919,11/73.78919 20/73.52072, 30/73.78559, 40/73.51532, 50/73.38198




new_df = cl_GW2

set.seed(22)


# Run the Model
brt_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))

  # Feature Selection
  fs_df <- mRMR.data(data = subtemp)
  out_c = mRMR.classic(data = fs_df, 
                     target_indices= ncol(subtemp), # Index of the target
                     feature_count = FC) # Number of features to be selected 
  fs_ind = unlist(solutions(out_c))
  fs_train = subtemp[,c(ncol(subtemp),fs_ind)]
  fs_test = subtest[,c(ncol(subtemp),fs_ind)]

  TRAINERS[[i]] = fs_train
  TESTERS[[i]] = fs_test
  
  # Run the model
  brt_model[[i]] = gbm(formula = Viol.Class ~ .,
               distribution = "gaussian",
               data = fs_train,
               n.trees = NTREES,
               interaction.depth=intdpth,
               shrinkage = shrk, # initially .01
               n.minobsinnode = NODES) # initially 20
  }
end_time <- Sys.time(); end_time - start_time
# Time difference of 17.50918 secs, with n.trees = 1000


pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
mean(pcc)
mean(sens)
mean(spec)


```

# Results
```{r}
# saveRDS(brt_model, file=paste0(out_dir,'brt_model_CV_class_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_brt_CV_class_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_brt_CV_class_GW_hypoth.rds'))

# brt_model <- readRDS(paste0(out_dir, 'brt_model_CV_class_GW_hypoth.rds'))
# TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_brt_CV_class_GW_hypoth.rds'))
# TESTERS <- readRDS(paste0(out_dir, 'TESTERS_brt_CV_class_GW_hypoth.rds'))


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TRAINERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){
  cl_pred = predict(brt_model[[i]],
                  newdata = TESTERS[[i]],
                  n.trees=NTREES, 
                  type='response')

  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)
```


# ANN - CV - Balanced
```{r}

library(AMORE)


#  found I need  to remove PctAlkIntruVolCat & MineDensCatRp100 & CatPctFull which have <=3 uniq values
new_df = subset(cl_GW2, select=-c(PctAlkIntruVolCat))
#new_df= cl_GW2    

# Run neuralnet
set.seed(22)
npreds = ncol(new_df)-1

#**********************************************************************
#**********************************************************************

# Found need to have the input layer be same as number of predictor variables 
ann <- newff(c(npreds,20,1), # round((1/4)*npreds)
             learning.rate.global=0.001, 
             momentum.global=0.05, # 0.05 initially
             error.criterium="LMS", Stao=NA, hidden.layer="tansig", 
              method="ADAPTgdwm") # create a neural network
# i = 10, for 0.05/0.05!!! WORKS!!! With Hidden = 10, seed = 22
# i = 10, for 0.001/0.05!!! WORKS!!! With Hidden = 10, seed = 22



# Run the Model
ANN_model = list()
In_TRAINERS = list() # keep list of training sets used
Target_TRAINERS = list() # keep list of testing sets used
In_TESTERS = list() # keep list of training sets used
Target_TESTERS = list() # keep list of testing sets used
train_target_orig = list() # keep list of training TARGETS used
test_target_orig = list() # keep list of testing TARGETS used
start_time <- Sys.time()
for (i in 1:10){
  # The next lines of code are for balancing 
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  sub = rbind(viol,noviol) # Combine the data back together

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  temp2 = temp
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows

  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # Extract Predictors and Response Variables
  input <- sub[,c(1:(ncol(sub)-2),ncol(sub))] 
  target <- sub[,c((ncol(sub)-1),ncol(sub))]  
  npreds = ncol(sub)-1

  # select out the 90% training & 10% test datasets based on CV_category
  train_input = input[input$CV_category != i,]
  train_input = subset(train_input, select=-c(CV_category))
  train_target = target[target$CV_category != i,]
  train_target = subset(train_target, select=-c(CV_category))
  
  test_input = input[input$CV_category == i,]
  test_input = subset(test_input, select=-c(CV_category))
  test_target = target[target$CV_category == i,]
  test_target = subset(test_target, select=-c(CV_category))
  
  # Scale the Data
  for (k in 1:ncol(train_input)) { 
    train_input[,k] <-(train_input[,k]-min(train_input[,k])) /
    (max(train_input[,k])-min(train_input[,k])) }
  for (k in 1:ncol(test_input)) { 
    test_input[,k] <-(test_input[,k]-min(test_input[,k])) /
    (max(test_input[,k])-min(test_input[,k])) }
  # replace NaN with Zero because that's what is produced if only zeros orig.
  train_input[is.na(train_input)] <- 0
  test_input[is.na(test_input)] <- 0

  #train_input = train_input[,c(1,bots:tops)] #******************************
  #test_input = test_input[,c(1,bots:tops)]#*********************************
  In_TRAINERS[[i]] = train_input
  Target_TRAINERS[[i]] = train_target
  In_TESTERS[[i]] = test_input
  Target_TESTERS[[i]] = test_target
  
  # Get original/unnormalized response variable
  train_target_orig1 = sub[sub$CV_category != i,]
  train_target_orig1 = train_target_orig1[,(ncol(sub)-1)]
  test_target_orig1 = sub[sub$CV_category == i,]
  test_target_orig1 = test_target_orig1[,(ncol(sub)-1)]
  train_target_orig[[i]] = train_target_orig1
  test_target_orig[[i]] = test_target_orig1
  
  # Run the model  
  ANN_model[[i]] <- train(ann, train_input, train_target, show.step=1, n.shows=100) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 29.267 mins, 100 n.shows 

# saveRDS(ANN_model,
#         file=paste0(out_dir,'ANN_model_h20_CV_class_GW_bal.rds'))
# saveRDS(In_TRAINERS,
#         file=paste0(out_dir,'In_TRAINERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(Target_TRAINERS,
#         file=paste0(out_dir,'Target_TRAINERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(In_TESTERS,
#         file=paste0(out_dir,'In_TESTERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(Target_TESTERS,
#         file=paste0(out_dir,'Target_TESTERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(train_target_orig,
#         file=paste0(out_dir,'train_target_orig_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(test_target_orig,
#         file=paste0(out_dir,'test_target_orig_ANN_h20_CV_class_GW_bal.rds'))



ANN_model <- readRDS(paste0(out_dir, 'ANN_model_h20_CV_class_GW_bal.rds'))


test = data.frame(Var = names(train_input), Uniq = 1)
for (i in 1:ncol(train_input)){
  test[i,2] = length(unique(train_input[,i]))
}





#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- train_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TRAINERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(In_TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- test_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(In_TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){

  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  cl_pred = as.numeric(ypred1)
  
  AUC[i] = roc.area(cl_pred, test_target_orig[[i]]) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)


```

# ANN - CV - Hypoth
```{r}

library(AMORE)


#  found I need  to remove PctAlkIntruVolCat & MineDensCatRp100 & CatPctFull which have <=3 uniq values
new_df = subset(cl_GW2, select=-c(PctAlkIntruVolCat))
 
# Tyring with Hypothesized variables
new_df = new_df[,hypo_vars]    

# Run neuralnet
set.seed(22)
npreds = ncol(new_df)-1

#**********************************************************************
#**********************************************************************

# Found need to have the input layer be same as number of predictor variables 
hiddenN = 20
ann <- newff(c(npreds,hiddenN,1), # round((1/4)*npreds)
             learning.rate.global=0.001, 
             momentum.global=0.05, # 0.05 initially
             error.criterium="LMS", Stao=NA, hidden.layer="tansig", 
              method="ADAPTgdwm") # create a neural network
# i = 10, for 0.05/0.05!!! WORKS!!! With Hidden = 10, seed = 22
# i = 10, for 0.001/0.05!!! WORKS!!! With Hidden = 10, seed = 22



# Run the Model
ANN_model = list()
In_TRAINERS = list() # keep list of training sets used
Target_TRAINERS = list() # keep list of testing sets used
In_TESTERS = list() # keep list of training sets used
Target_TESTERS = list() # keep list of testing sets used
train_target_orig = list() # keep list of training TARGETS used
test_target_orig = list() # keep list of testing TARGETS used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  temp2 = temp
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows

  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
    
  # Extract Predictors and Response Variables
  input <- sub[,2:(ncol(sub))] 
  target <- sub[,c(1,ncol(sub))]  
  npreds = ncol(sub)-1

  # select out the 90% training & 10% test datasets based on CV_category
  train_input = input[input$CV_category != i,]
  train_input = subset(train_input, select=-c(CV_category))
  train_target = target[target$CV_category != i,]
  train_target = subset(train_target, select=-c(CV_category))
  
  test_input = input[input$CV_category == i,]
  test_input = subset(test_input, select=-c(CV_category))
  test_target = target[target$CV_category == i,]
  test_target = subset(test_target, select=-c(CV_category))
  
  # Scale the Data
  for (k in 1:ncol(train_input)) { 
    train_input[,k] <-(train_input[,k]-min(train_input[,k])) /
    (max(train_input[,k])-min(train_input[,k])) }
  for (k in 1:ncol(test_input)) { 
    test_input[,k] <-(test_input[,k]-min(test_input[,k])) /
    (max(test_input[,k])-min(test_input[,k])) }
  # replace NaN with Zero because that's what is produced if only zeros orig.
  train_input[is.na(train_input)] <- 0
  test_input[is.na(test_input)] <- 0

  #train_input = train_input[,c(1,bots:tops)] #******************************
  #test_input = test_input[,c(1,bots:tops)]#*********************************
  In_TRAINERS[[i]] = train_input
  Target_TRAINERS[[i]] = train_target
  In_TESTERS[[i]] = test_input
  Target_TESTERS[[i]] = test_target
  
  # Get original/unnormalized response variable
  train_target_orig1 = sub[sub$CV_category != i,]
  train_target_orig1 = train_target_orig1[,1]
  test_target_orig1 = sub[sub$CV_category == i,]
  test_target_orig1 = test_target_orig1[,1]
  train_target_orig[[i]] = train_target_orig1
  test_target_orig[[i]] = test_target_orig1
  
  # Run the model  
  ANN_model[[i]] <- train(ann, train_input, train_target, show.step=1, n.shows=1000) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.397325 mins, hidden=20, balanced, 

# saveRDS(ANN_model,
#         file=paste0(out_dir,'ANN_model_h20_CV_class_GW_hypoth.rds'))
# saveRDS(In_TRAINERS,
#         file=paste0(out_dir,'In_TRAINERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(Target_TRAINERS,
#       file=paste0(out_dir,'Target_TRAINERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(In_TESTERS,
#         file=paste0(out_dir,'In_TESTERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(Target_TESTERS,
#         file=paste0(out_dir,'Target_TESTERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(train_target_orig,
#     file=paste0(out_dir,'train_target_orig_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(test_target_orig,
#         file=paste0(out_dir,'test_target_orig_ANN_h20_CV_class_hypoth.rds'))





 
ANN_model <- readRDS(paste0(out_dir, 'ANN_model_h10_CV_PercGW_balanced.rds'))


test = data.frame(Var = names(train_input), Uniq = 1)
for (i in 1:ncol(train_input)){
  test[i,2] = length(unique(train_input[,i]))
}





#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- train_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TRAINERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- test_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(In_TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){

  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  cl_pred = as.numeric(ypred1)
  
  AUC[i] = roc.area(cl_pred, test_target_orig[[i]]) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)


```



# BN - CV - Balanced
```{r}
library(bnlearn)

set.seed(22)
# Run the Model
BN_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = cl_GW2[cl_GW2$Viol.Class == 0,] # separate cats without viols
  viol = cl_GW2[cl_GW2$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  res <- hc(subtemp)
  BN_model[[i]]  <- bn.fit(res, data = subtemp)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of Time difference of 1.968158 hours!!

# saveRDS(BN_model, file=paste0(out_dir, 'BN_model_CV_class_GW_balanced.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_BN_CV_class_GW_balanced.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_BN_CV_class_GW_balanced.rds'))

BN_model <- readRDS(paste0(out_dir, 'BN_model_CV_class_GW_balanced.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_BN_CV_class_GW_balanced.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_BN_CV_class_GW_balanced.rds'))


#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
start_time <- Sys.time()
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(BN_model[[i]],node="Viol.Class",
                  data=TRAINERS[[i]],method = "bayes-lw")
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)
end_time <- Sys.time(); end_time - start_time
# Time difference of 26.1655 mins

# TESTING DATASET
start_time <- Sys.time()
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(BN_model[[i]],node="Viol.Class",
                  data=TESTERS[[i]],method = "bayes-lw")
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.900375 mins

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

start_time <- Sys.time()
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(BN_model[[i]],node="Viol.Class",
                  data=TESTERS[[i]],method = "bayes-lw")
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.856861 mins

noquote(paste("AUC for RFC= ", AUC_all)) # 0.816252681204384

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

```

# BN - CV - Hypothesized
```{r}
library(bnlearn)

# Tyring with Hypothesized variables
new_df = cl_GW2[,hypo_vars]


set.seed(22)
# Run the Model
BN_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  res <- hc(subtemp)
  BN_model[[i]]  <- bn.fit(res, data = subtemp)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.283 mins

# saveRDS(BN_model, file=paste0(out_dir, 'BN_model_CV_class_GW_hypoth.rds'))
# saveRDS(TRAINERS, file=paste0(out_dir,'TRAINERS_BN_CV_class_GW_hypoth.rds'))
# saveRDS(TESTERS, file=paste0(out_dir,'TESTERS_BN_CV_class_GW_hypoth.rds'))

BN_model <- readRDS(paste0(out_dir, 'BN_model_CV_class_GW_hypoth.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_BN_CV_class_GW_hypoth.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_BN_CV_class_GW_hypoth.rds'))


#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
start_time <- Sys.time()
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(BN_model[[i]],node="Viol.Class",
                  data=TRAINERS[[i]],method = "bayes-lw")
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)
end_time <- Sys.time(); end_time - start_time
# Time difference of 1.346014 mins

# TESTING DATASET
start_time <- Sys.time()
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(BN_model[[i]],node="Viol.Class",
                  data=TESTERS[[i]],method = "bayes-lw")
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)
end_time <- Sys.time(); end_time - start_time
# Time difference of 10.13729 secs

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

start_time <- Sys.time()
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(BN_model[[i]],node="Viol.Class",
                  data=TESTERS[[i]],method = "bayes-lw")
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.681849 mins

noquote(paste("AUC for RFC= ", AUC_all)) # 0.816252681204384

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

```


# GAM - CV - Classification
```{r}

# detach(package:mgcv)
library(gam)
#library(mgcv)

# Create formula to use in GAM Model 
# preds= noquote(paste(paste(colnames(cv_df[,2:ncol(cv_df)]),collapse=")+s(")))
# formula = noquote(paste0("perc_viol~s(" ,preds,")"))

# Used this after got an error running the model saying variables have to have more than 3 unique variables. 
test = data.frame(Var = names(subtemp), Uniq = 1)
for (i in 1:ncol(subtemp)){
  test[i,2] = length(unique(subtemp[,i]))
}

any(is.na(subtemp))


# As a results of above code, found I need  to remove PctAlkIntruVolCat & MineDensCatRp100 & CatPctFull which have <=3 uniq values
new_df = cl_GW2
new_df = subset(new_df, select=-c(PctAlkIntruVolCat,
                                   MineDensCatRp100,
                                    CatPctFull,
                                   PctSalLakeCat
                                   # DamNIDStorCat, wwtp_minor_km2Ws,
                                   # Tmean08Ws, Tmean09Cat,
                                   # PctSSACat
                                   ))

set.seed(25)
# Run the Model
GAM_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  #subtemp = subtemp[,c(1:200,ncol(subtemp))]
  #subtest = subtest[,c(1:200,ncol(subtemp))]
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  preds = noquote(paste(paste(colnames(subtemp[,2:(ncol(subtemp)-1)]),
                              collapse=")+s(")))
  formula = noquote(paste0("Viol.Class~s(" ,preds,")"))
  GAM_model[[i]] <- gam(as.formula(formula), data = subtemp, r.sq=T)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 12.09318 mins

# saveRDS(GAM_model,
#         file=paste0(out_dir, 'GAM_model_CV_class_GW_balanced.rds'))
# saveRDS(TRAINERS,
#         file=paste0(out_dir,'TRAINERS_GAM_CV_class_GW_balanced.rds'))
# saveRDS(TESTERS,
#         file=paste0(out_dir, 'TESTERS_GAM_CV_class_GW_balanced.rds'))

GAM_model <- readRDS(paste0(out_dir, 'GAM_model_CV_class_GW_balanced.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_GAM_CV_class_GW_balanced.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_GAM_CV_class_GW_balanced.rds'))


#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(GAM_model[[i]],newdata = TRAINERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)


# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(GAM_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

start_time <- Sys.time()
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(GAM_model[[i]],newdata = TESTERS[[i]])
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.681849 mins

noquote(paste("AUC for RFC= ", AUC_all)) # 0.816252681204384

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

```

# GAM - CV - Hypothesized
```{r}

# detach(package:mgcv)
library(gam)
#library(mgcv)

# Create formula to use in GAM Model 
# preds= noquote(paste(paste(colnames(cv_df[,2:ncol(cv_df)]),collapse=")+s(")))
# formula = noquote(paste0("perc_viol~s(" ,preds,")"))

# Used this after got an error running the model saying variables have to have more than 3 unique variables. 
test = data.frame(Var = names(subtemp), Uniq = 1)
for (i in 1:ncol(subtemp)){
  test[i,2] = length(unique(subtemp[,i]))
}

any(is.na(subtemp))




# Tyring with Hypothesized variables
new_df = cl_GW2[,hypo_vars]
# As a results of above code, found I need  to remove PctAlkIntruVolCat & MineDensCatRp100 & CatPctFull which have <=3 uniq values
new_df = subset(new_df, select=-c(PctAlkIntruVolCat,
                                   MineDensCatRp100,
                                    CatPctFull,
                                   PctSalLakeCat
                                   # DamNIDStorCat, wwtp_minor_km2Ws,
                                   # Tmean08Ws, Tmean09Cat,
                                   # PctSSACat
                                   ))

set.seed(25)
# Run the Model
GAM_model = list()
TRAINERS = list() # keep list of training sets used
TESTERS = list() # keep list of testing sets used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp # **  Comment out if switching back to old method
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows
  sub$CV_category = temp2 # Add the category to the dataframe
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe
  
  # select out the 90% training & 10% test datasets based on CV_category
  subtemp = sub[sub$CV_category != i,]
  subtemp = subset(subtemp, select=-c(CV_category))
  subtest = sub[sub$CV_category == i,]
  subtest = subset(subtest, select=-c(CV_category))
  #subtemp = subtemp[,c(1:200,ncol(subtemp))]
  #subtest = subtest[,c(1:200,ncol(subtemp))]
  TRAINERS[[i]] = subtemp
  TESTERS[[i]] = subtest
  
  # Run the model
  preds = noquote(paste(paste(colnames(subtemp[,2:(ncol(subtemp)-1)]),
                              collapse=")+s(")))
  formula = noquote(paste0("Viol.Class~s(" ,preds,")"))
  GAM_model[[i]] <- gam(as.formula(formula), data = subtemp, r.sq=T)
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 3.306925 mins

# saveRDS(GAM_model, 
#         file=paste0(out_dir, 'GAM_model_CV_class_GW_hypoth.rds'))
# saveRDS(TRAINERS, 
#         file=paste0(out_dir,'TRAINERS_GAM_CV_class_GW_hypoth.rds'))
# saveRDS(TESTERS, 
#         file=paste0(out_dir, 'TESTERS_GAM_CV_class_GW_hypoth.rds'))

GAM_model <- readRDS(paste0(out_dir, 'GAM_model_CV_class_GW_hypoth.rds'))
TRAINERS <- readRDS(paste0(out_dir, 'TRAINERS_GAM_CV_class_GW_hypoth.rds'))
TESTERS <- readRDS(paste0(out_dir, 'TESTERS_GAM_CV_class_GW_hypoth.rds'))


#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TRAINERS[[i]]$Viol.Class
  ypred1 = predict(GAM_model[[i]],newdata = TRAINERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)


# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- TESTERS[[i]]$Viol.Class
  ypred1 = predict(GAM_model[[i]],newdata = TESTERS[[i]])
  ypred = ifelse(ypred1 > 0.5,1,0)
  n <- nrow(TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}

start_time <- Sys.time()
AUC = c()
for (i in 1:10){ 
  cl_pred = predict(GAM_model[[i]],newdata = TESTERS[[i]])
  AUC[i] = roc.area(cl_pred, TESTERS[[i]]$Viol.Class) 
}
AUC_all = mean(AUC)
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.681849 mins

noquote(paste("AUC for RFC= ", AUC_all)) # 0.816252681204384

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)

```



#***

# Orig.ANN - CV - Classificiation
```{r}

library(AMORE)


#  found I need  to remove PctAlkIntruVolCat & MineDensCatRp100 & CatPctFull which have <=3 uniq values
new_df = subset(cl_GW2, select=-c(PctAlkIntruVolCat))
#new_df= cl_GW2    

# Run neuralnet
set.seed(22)
npreds = ncol(new_df)-1

#**********************************************************************
#**********************************************************************

# Found need to have the input layer be same as number of predictor variables 
ann <- newff(c(npreds,20,1), # round((1/4)*npreds)
             learning.rate.global=0.001, 
             momentum.global=0.05, # 0.05 initially
             error.criterium="LMS", Stao=NA, hidden.layer="tansig", 
              method="ADAPTgdwm") # create a neural network
# i = 10, for 0.05/0.05!!! WORKS!!! With Hidden = 10, seed = 22
# i = 10, for 0.001/0.05!!! WORKS!!! With Hidden = 10, seed = 22



# Run the Model
ANN_model = list()
In_TRAINERS = list() # keep list of training sets used
Target_TRAINERS = list() # keep list of testing sets used
In_TESTERS = list() # keep list of training sets used
Target_TESTERS = list() # keep list of testing sets used
train_target_orig = list() # keep list of training TARGETS used
test_target_orig = list() # keep list of testing TARGETS used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # Extract Predictors and Response Variables
  input <- sub[,1:(ncol(sub)-1)] 
  target1 <- sub[,ncol(sub)]  
  npreds = ncol(sub)-1

  # Scale the Data
  for (k in 1:ncol(input)) { input[,k] <-(input[,k]-min(input[,k])) /
    (max(input[,k])-min(input[,k])) }
  #target <-(target1-min(target1)) / (max(target1)-min(target1))
  target = as.data.frame(target1)
  
  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows

  input$CV_category = temp2 # Add the category to the dataframe
  target$CV_category = temp2 # Add the category to the dataframe
  sub$CV_category = temp2 # Add the category to the dataframe

  input=input[sample(nrow(input)),]  # randomly change row order of dataframe
  target=target[sample(nrow(target)),] # randomly change row order of df
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # select out the 90% training & 10% test datasets based on CV_category
  train_input = input[input$CV_category != i,]
  train_input = subset(train_input, select=-c(CV_category))
  train_target = target[target$CV_category != i,]
  train_target = subset(train_target, select=-c(CV_category))
  
  test_input = input[input$CV_category == i,]
  test_input = subset(test_input, select=-c(CV_category))
  test_target = target[target$CV_category == i,]
  test_target = subset(test_target, select=-c(CV_category))
  
  In_TRAINERS[[i]] = train_input
  Target_TRAINERS[[i]] = train_target

  In_TESTERS[[i]] = test_input
  Target_TESTERS[[i]] = test_target
  
  # Get original/unnormalized response variable
  train_target_orig1 = sub[sub$CV_category != i,]
  train_target_orig1 = train_target_orig1[,(ncol(sub)-1)]
  test_target_orig1 = sub[sub$CV_category == i,]
  test_target_orig1 = test_target_orig1[,(ncol(sub)-1)]
  train_target_orig[[i]] = train_target_orig1
  test_target_orig[[i]] = test_target_orig1
  
  # Run the model  
  ANN_model[[i]] <- train(ann, train_input, train_target, show.step=1, n.shows=1000) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 6.814977 mins, hidden=20, balanced, 

# saveRDS(ANN_model,
#         file=paste0(out_dir,'ANN_model_h20_CV_class_GW_bal.rds'))
# saveRDS(In_TRAINERS,
#         file=paste0(out_dir,'In_TRAINERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(Target_TRAINERS,
#         file=paste0(out_dir,'Target_TRAINERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(In_TESTERS,
#         file=paste0(out_dir,'In_TESTERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(Target_TESTERS,
#         file=paste0(out_dir,'Target_TESTERS_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(train_target_orig,
#         file=paste0(out_dir,'train_target_orig_ANN_h20_CV_class_GW_bal.rds'))
# saveRDS(test_target_orig,
#         file=paste0(out_dir,'test_target_orig_ANN_h20_CV_class_GW_bal.rds'))



ANN_model <- readRDS(paste0(out_dir, 'ANN_model_h20_CV_class_GW_bal.rds'))


test = data.frame(Var = names(train_input), Uniq = 1)
for (i in 1:ncol(train_input)){
  test[i,2] = length(unique(train_input[,i]))
}





#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- train_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TRAINERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- test_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(In_TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)


#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){

  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  cl_pred = as.numeric(ypred1)
  
  AUC[i] = roc.area(cl_pred, test_target_orig[[i]]) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)


```

# Orig.ANN - CV - Hypoth
```{r}

library(AMORE)


#  found I need  to remove PctAlkIntruVolCat & MineDensCatRp100 & CatPctFull which have <=3 uniq values
new_df = subset(cl_GW2, select=-c(PctAlkIntruVolCat))
 
# Tyring with Hypothesized variables
new_df = new_df[,hypo_vars]    

# Run neuralnet
set.seed(22)
npreds = ncol(new_df)-1

#**********************************************************************
#**********************************************************************

# Found need to have the input layer be same as number of predictor variables 
hiddenN = 20
ann <- newff(c(npreds,hiddenN,1), # round((1/4)*npreds)
             learning.rate.global=0.001, 
             momentum.global=0.05, # 0.05 initially
             error.criterium="LMS", Stao=NA, hidden.layer="tansig", 
              method="ADAPTgdwm") # create a neural network
# i = 10, for 0.05/0.05!!! WORKS!!! With Hidden = 10, seed = 22
# i = 10, for 0.001/0.05!!! WORKS!!! With Hidden = 10, seed = 22



# Run the Model
ANN_model = list()
In_TRAINERS = list() # keep list of training sets used
Target_TRAINERS = list() # keep list of testing sets used
In_TESTERS = list() # keep list of training sets used
Target_TESTERS = list() # keep list of testing sets used
train_target_orig = list() # keep list of training TARGETS used
test_target_orig = list() # keep list of testing TARGETS used
start_time <- Sys.time()
for (i in 1:10){
  # The next 5 lines of code are for balancing by random sampling of 0s
  noviol = new_df[new_df$Viol.Class == 0,] # separate cats without viols
  viol = new_df[new_df$Viol.Class > 0,] # separate cats with viols
  noviol2 = noviol[sample(nrow(noviol), nrow(viol)), ]# random sample rows
  sub = rbind(viol,noviol2) # Combine the data back together
  #sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # Extract Predictors and Response Variables
  input <- sub[,2:(ncol(sub))] 
  target1 <- sub[,1]  
  npreds = ncol(sub)-1

  # Scale the Data
  for (k in 1:ncol(input)) { input[,k] <-(input[,k]-min(input[,k])) /
    (max(input[,k])-min(input[,k])) }
  #target <-(target1-min(target1)) / (max(target1)-min(target1))
  target = as.data.frame(target1)
  
  # append the 1 through 10 categories
  temp = rep(c(1,2,3,4,5,6,7,8,9,10), ceiling(nrow(sub)/10))
  #temp2 <- temp[sample(1:length(temp))] # shuffle the numbers
  temp2 = temp
  temp2 = temp2[1:nrow(sub)] # cut off any extra rows

  input$CV_category = temp2 # Add the category to the dataframe
  target$CV_category = temp2 # Add the category to the dataframe
  sub$CV_category = temp2 # Add the category to the dataframe

  input=input[sample(nrow(input)),]  # randomly change row order of dataframe
  target=target[sample(nrow(target)),] # randomly change row order of df
  sub=sub[sample(nrow(sub)),]  # randomly change row order of dataframe

  # select out the 90% training & 10% test datasets based on CV_category
  train_input = input[input$CV_category != i,]
  train_input = subset(train_input, select=-c(CV_category))
  train_target = target[target$CV_category != i,]
  train_target = subset(train_target, select=-c(CV_category))
  
  test_input = input[input$CV_category == i,]
  test_input = subset(test_input, select=-c(CV_category))
  test_target = target[target$CV_category == i,]
  test_target = subset(test_target, select=-c(CV_category))
  
  In_TRAINERS[[i]] = train_input
  Target_TRAINERS[[i]] = train_target

  In_TESTERS[[i]] = test_input
  Target_TESTERS[[i]] = test_target
  
  # Get original/unnormalized response variable
  train_target_orig1 = sub[sub$CV_category != i,]
  train_target_orig1 = train_target_orig1[,1]
  test_target_orig1 = sub[sub$CV_category == i,]
  test_target_orig1 = test_target_orig1[,1]
  train_target_orig[[i]] = train_target_orig1
  test_target_orig[[i]] = test_target_orig1
  
  # Run the model  
  ANN_model[[i]] <- train(ann, train_input, train_target, show.step=1, n.shows=1000) 
}
end_time <- Sys.time(); end_time - start_time
# Time difference of 2.397325 mins, hidden=20, balanced, 

# saveRDS(ANN_model,
#         file=paste0(out_dir,'ANN_model_h20_CV_class_GW_hypoth.rds'))
# saveRDS(In_TRAINERS,
#         file=paste0(out_dir,'In_TRAINERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(Target_TRAINERS,
#       file=paste0(out_dir,'Target_TRAINERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(In_TESTERS,
#         file=paste0(out_dir,'In_TESTERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(Target_TESTERS,
#         file=paste0(out_dir,'Target_TESTERS_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(train_target_orig,
#     file=paste0(out_dir,'train_target_orig_ANN_h20_CV_class_GW_hypoth.rds'))
# saveRDS(test_target_orig,
#         file=paste0(out_dir,'test_target_orig_ANN_h20_CV_class_hypoth.rds'))





 
ANN_model <- readRDS(paste0(out_dir, 'ANN_model_h10_CV_PercGW_balanced.rds'))


test = data.frame(Var = names(train_input), Uniq = 1)
for (i in 1:ncol(train_input)){
  test[i,2] = length(unique(train_input[,i]))
}





#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

#**********************************************************************
#**********************************************************************

# Model evaluation: Percent Correctly Classified
# Out-of-Bag Accuracy Estimates
# Calculation of Percent Correctly Classified (PCC), 
# Sensitivity (Sens) = % of obsv in class 1 correctly classified
# Specificity (spec) = % of obsv in class 0 correctly classified
# G-Mean (balanced accuracy) = sqrt(sens X spec), (Anand etal 2010 or Dal Pazzo etal 2015)

# TRAINING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- train_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TRAINERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(TRAINERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_train = mean(pcc)
sens_train = mean(sens)
spec_train = mean(spec)

# TESTING DATASET
pcc = c()
sens = c()
spec = c()
for (i in 1:10){ 
  yobs <- test_target_orig[[i]]
  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  ypred2 = as.numeric(ypred1)
  ypred = ifelse(ypred2>0.5,1,0)
  n <- nrow(In_TESTERS[[i]])
  pcc[i]  <- 100*sum(yobs == ypred) / n
  sens[i] <- 100*sum(yobs == 1 & ypred == 1) / sum(yobs==1)
  spec[i] <- 100*sum(yobs == 0 & ypred == 0) / sum(yobs==0)
}
pcc_test = mean(pcc)
sens_test = mean(sens)
spec_test = mean(spec)
gmean_test = sqrt(sens_test*spec_test)

#***********************************************************************
#***********************************************************************
# Model evaluation: Area Under the Curve (AUC)
#Function for AUC
roc.area<-function(probp,prabs) {
  presnt<-probp[prabs>.5];
  abb<-probp[prabs<.5];
  sumexc <- sapply(presnt, FUN = function(x,cc)(sum(as.numeric(x > cc)) + 0.5 * sum(as.numeric(x == cc))),cc=abb)
  sum(sumexc)/(length(presnt)*length(abb));
}


AUC = c()
for (i in 1:10){

  ypred1 = sim(ANN_model[[i]]$net, In_TESTERS[[i]]) 
  cl_pred = as.numeric(ypred1)
  
  AUC[i] = roc.area(cl_pred, test_target_orig[[i]]) 
}
AUC_all = mean(AUC)

noquote(paste("AUC for RFC= ", AUC_all)) # 0.857365895311578

#*******************************************************
# Make Table of Results
CV_Results = data.frame(pcc_test = pcc_test,
                        sens_test = sens_test,
                        spec_test = spec_test,
                        AUC_all = AUC_all)
                       
CV_Results$pcc_test = round(CV_Results$pcc_test,1)
CV_Results$sens_test = round(CV_Results$sens_test,1)
CV_Results$spec_test = round(CV_Results$spec_test,1)
CV_Results$AUC_all = round(CV_Results$AUC_all,2)


```

#***
